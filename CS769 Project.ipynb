{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCGGtefGvAWF"
   },
   "source": [
    "# Python Code\n",
    "\n",
    "Here we are importing libraries and creating necessary funcitions,\n",
    "and following next cell contains main function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REHHeV61CkBr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 64910,
     "status": "ok",
     "timestamp": 1651567844668,
     "user": {
      "displayName": "Deeti Yeshash Chandra",
      "userId": "09871344677158725097"
     },
     "user_tz": -330
    },
    "id": "zHh0GY7nB7Be"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from queue import PriorityQueue\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from utils.custom_dataset import load_std_regress_data,load_dataset_custom\n",
    "#from utils.Create_Slices import get_slices\n",
    "#from model.LinearRegression import RegressionNet, LogisticNet\n",
    "#from model.SELCON import FindSubset_Vect_No_ValLoss as FindSubset_Vect, FindSubset_Vect_TrnLoss\n",
    "#from model.facility_location import run_stochastic_Facloc\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class FindSubset_Vect(object):\n",
    "    def __init__(self, x_trn, y_trn, x_val, y_val,model,loss,device,delta,lr,lam,batch):\n",
    "        \n",
    "        self.x_trn = x_trn\n",
    "        self.y_trn = y_trn\n",
    "        #self.trn_batch = trn_batch\n",
    "\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = loss \n",
    "        self.device = device\n",
    "\n",
    "        self.delta = delta\n",
    "        self.lr = lr\n",
    "        self.lam = lam\n",
    "        #self.optimizer = optimizer\n",
    "        self.batch_size = batch\n",
    "\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        np.random.seed(42)\n",
    "\n",
    "\n",
    "    def precompute(self,f_pi_epoch,p_epoch,alphas):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "\n",
    "        main_optimizer = torch.optim.Adam([\n",
    "                {'params': self.model.parameters()}], lr=self.lr)\n",
    "                \n",
    "        dual_optimizer = torch.optim.Adam([{'params': alphas}], lr=self.lr)\n",
    "\n",
    "        print(\"starting Pre compute\")\n",
    "        #alphas = torch.rand_like(self.delta,requires_grad=True) \n",
    "\n",
    "        #print(alphas)\n",
    "\n",
    "        #scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[10,20,40,100],\\\n",
    "        #    gamma=0.5) #[e*2 for e in change]\n",
    "\n",
    "        #alphas.requires_grad = False\n",
    "        loader_val = DataLoader(CustomDataset(self.x_val, self.y_val, device = self.device, transform=None),\\\n",
    "            shuffle=False, batch_size=self.batch_size)\n",
    "            \n",
    "\n",
    "        #Compute F_phi\n",
    "        #for i in range(f_pi_epoch):\n",
    "\n",
    "        prev_loss = 1000\n",
    "        stop_count = 0\n",
    "        i=0\n",
    "        \n",
    "        while(True):\n",
    "            \n",
    "            main_optimizer.zero_grad()\n",
    "            \n",
    "            '''l2_reg = 0\n",
    "            for param in self.model.parameters():\n",
    "                l2_reg += torch.norm(param)'''\n",
    "\n",
    "            #l = [torch.flatten(p) for p in main_model.parameters()]\n",
    "            #flat = torch.cat(l)\n",
    "            #l2_reg = torch.sum(flat*flat)\n",
    "            \n",
    "            constraint = 0. \n",
    "            for batch_idx in list(loader_val.batch_sampler):\n",
    "                    \n",
    "                inputs, targets = loader_val.dataset[batch_idx]\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "                val_out = self.model(inputs)\n",
    "                constraint += self.criterion(val_out, targets)\n",
    "                \n",
    "            constraint /= len(loader_val.batch_sampler)\n",
    "            constraint = constraint - self.delta\n",
    "            multiplier = alphas*constraint #torch.dot(alphas,constraint)\n",
    "\n",
    "            loss = multiplier\n",
    "            self.F_phi = loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            main_optimizer.step()\n",
    "            #scheduler.step()\n",
    "            \n",
    "            '''for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            alphas.requires_grad = True'''\n",
    "\n",
    "            dual_optimizer.zero_grad()\n",
    "\n",
    "            constraint = 0.\n",
    "            for batch_idx in list(loader_val.batch_sampler):\n",
    "                    \n",
    "                inputs, targets = loader_val.dataset[batch_idx]\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "                val_out = self.model(inputs)\n",
    "                constraint += self.criterion(val_out, targets)\n",
    "            \n",
    "            constraint /= len(loader_val.batch_sampler)\n",
    "            constraint = constraint - self.delta\n",
    "            multiplier = -1.0*alphas*constraint #torch.dot(-1.0*alphas,constraint)\n",
    "            \n",
    "            multiplier.backward()\n",
    "            \n",
    "            dual_optimizer.step()\n",
    "\n",
    "            alphas.requires_grad = False\n",
    "            alphas.clamp_(min=0.0)\n",
    "            alphas.requires_grad = True\n",
    "            #print(alphas)\n",
    "\n",
    "            '''for param in self.model.parameters():\n",
    "                param.requires_grad = True'''\n",
    "\n",
    "            if loss.item() <= 0.:\n",
    "                break\n",
    "\n",
    "            #if i>= f_pi_epoch:\n",
    "            #    break\n",
    "\n",
    "            if abs(prev_loss - loss.item()) <= 1e-1 and stop_count >= 5:\n",
    "                break \n",
    "            elif abs(prev_loss - loss.item()) <= 1e-1:\n",
    "                stop_count += 1\n",
    "            else:\n",
    "                stop_count = 0\n",
    "\n",
    "            prev_loss = loss.item()\n",
    "            i+=1\n",
    "\n",
    "            #if i % 50 == 0:\n",
    "            #    print(loss.item(),alphas,constraint)\n",
    "\n",
    "        print(\"Finishing F phi\")\n",
    "\n",
    "        if loss.item() <= 0.:\n",
    "            alphas = torch.zeros_like(alphas)\n",
    "\n",
    "        print(loss.item())\n",
    "\n",
    "        l = [torch.flatten(p) for p in self.model.state_dict().values()]\n",
    "        flat = torch.cat(l).detach().clone()\n",
    "        \n",
    "        '''main_optimizer = torch.optim.Adam([{'params': self.model.parameters()}], lr=self.lr)\n",
    "        \n",
    "        alphas = torch.ones(alphas.shape) * 1e-4\n",
    "        x_val_ext = torch.cat((self.x_val,torch.ones(self.x_val.shape[0],device=self.device).view(-1,1))\\\n",
    "                ,dim=1)\n",
    "\n",
    "        yTy = torch.dot(self.y_val,self.y_val)\n",
    "        \n",
    "        #yTX = alphas*torch.matmul(self.y_val.view(1,-1),x_val_ext)\n",
    "\n",
    "        XTX = torch.inverse(alphas*torch.matmul(x_val_ext.T,x_val_ext))\n",
    "\n",
    "        XTy = alphas*torch.matmul(x_val_ext.T,self.y_val.view(-1,1))\n",
    "\n",
    "        accum = torch.matmul(XTX, XTy) #torch.matmul(XTy,XTX)\n",
    "\n",
    "        flat = alphas*yTy - alphas*self.delta - accum'''\n",
    "        \n",
    "        self.F_values = torch.zeros(len(self.x_trn),device=self.device)\n",
    "\n",
    "        device_new = self.device #\"cuda:2\"#self.device #\n",
    "        beta1,beta2 = main_optimizer.param_groups[0]['betas']\n",
    "        #main_optimizer.param_groups[0]['eps']\n",
    "\n",
    "        loader_tr = DataLoader(CustomDataset_WithId(self.x_trn, self.y_trn,\\\n",
    "            transform=None),shuffle=False,batch_size=self.batch_size*20)\n",
    "\n",
    "        loader_val = DataLoader(CustomDataset(self.x_val, self.y_val,device = self.device,transform=None),\\\n",
    "            shuffle=False,batch_size=self.batch_size*20)\n",
    "        \n",
    "        for batch_idx in list(loader_tr.batch_sampler):\n",
    "\n",
    "            inputs, targets, idxs = loader_tr.dataset[batch_idx]\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "            ele_delta = self.delta.repeat(targets.shape[0]).to(self.device)\n",
    "        \n",
    "            weights = flat.view(1,-1).repeat(targets.shape[0], 1)\n",
    "            ele_alphas = alphas.detach().repeat(targets.shape[0]).to(self.device)\n",
    "            #print(weights.shape)\n",
    "\n",
    "            exp_avg_w = torch.zeros_like(weights)\n",
    "            exp_avg_sq_w = torch.zeros_like(weights)\n",
    "\n",
    "            #exp_avg_a = torch.zeros_like(ele_alphas)\n",
    "            #exp_avg_sq_a = torch.zeros_like(ele_alphas)\n",
    "\n",
    "            exten_inp = torch.cat((inputs,torch.ones(inputs.shape[0],device=self.device).view(-1,1))\\\n",
    "                ,dim=1)\n",
    "\n",
    "            bias_correction1 = 1.0 \n",
    "            bias_correction2 = 1.0 \n",
    "\n",
    "            for i in range(p_epoch):\n",
    "\n",
    "                trn_loss_g = torch.sum(exten_inp*weights,dim=1) - targets\n",
    "                fin_trn_loss_g = exten_inp*2*trn_loss_g[:,None]\n",
    "\n",
    "                #no_bias = weights.clone()\n",
    "                #no_bias[-1,:] = torch.zeros(weights.shape[0])\n",
    "                \n",
    "                weight_grad = fin_trn_loss_g+ 2*self.lam*\\\n",
    "                    torch.cat((weights[:,:-1], torch.zeros((weights.shape[0],1),device=self.device)),dim=1)\n",
    "                #+fin_val_loss_g*ele_alphas[:,None]\n",
    "\n",
    "                exp_avg_w.mul_(beta1).add_(1.0 - beta1, weight_grad)\n",
    "                exp_avg_sq_w.mul_(beta2).addcmul_(1.0 - beta2, weight_grad, weight_grad)\n",
    "                denom = exp_avg_sq_w.sqrt().add_(main_optimizer.param_groups[0]['eps'])\n",
    "\n",
    "                bias_correction1 *= beta1\n",
    "                bias_correction2 *= beta2\n",
    "                step_size = (self.lr) * math.sqrt(1.0-bias_correction2) / (1.0-bias_correction1)\n",
    "                weights.addcdiv_(-step_size, exp_avg_w, denom)\n",
    "                \n",
    "            val_losses = 0.\n",
    "            for batch_idx_val in list(loader_val.batch_sampler):\n",
    "                    \n",
    "                inputs_val, targets_val = loader_val.dataset[batch_idx_val]\n",
    "                inputs_val, targets_val = inputs_val.to(self.device), targets_val.to(self.device)\n",
    "\n",
    "                exten_val = torch.cat((inputs_val,torch.ones(inputs_val.shape[0],device=self.device).view(-1,1)),dim=1)\n",
    "                \n",
    "                exten_val_y = torch.mean(targets_val).repeat(min(self.batch_size*20,targets.shape[0]))\n",
    "\n",
    "                val_loss = torch.sum(weights*torch.mean(exten_val,dim=0),dim=1) - exten_val_y\n",
    "\n",
    "                val_losses+= val_loss*val_loss #torch.mean(val_loss*val_loss,dim=0)\n",
    "            \n",
    "            reg = torch.sum(weights[:,:-1]*weights[:,:-1],dim=1)\n",
    "            trn_loss = torch.sum(exten_inp*weights,dim=1) - targets\n",
    "\n",
    "            self.F_values[idxs] = trn_loss*trn_loss+ self.lam*reg +torch.max(torch.zeros_like(ele_alphas),\\\n",
    "                (val_losses/len(loader_val.batch_sampler)-ele_delta)*ele_alphas)\n",
    "\n",
    "        print(self.F_values[:10])\n",
    "\n",
    "        #self.F_values = self.F_values - max(loss.item(),0.) \n",
    "\n",
    "        print(\"Finishing Element wise F\")\n",
    "\n",
    "\n",
    "    def return_subset(self,theta_init,p_epoch,curr_subset,alphas,budget,batch,\\\n",
    "        step,w_exp_avg,w_exp_avg_sq,a_exp_avg,a_exp_avg_sq):\n",
    "        \n",
    "        m_values = self.F_values.detach().clone() #torch.zeros(len(self.x_trn))\n",
    "        \n",
    "        self.model.load_state_dict(theta_init)\n",
    "\n",
    "        loader_tr = DataLoader(CustomDataset_WithId(self.x_trn[curr_subset], self.y_trn[curr_subset],\\\n",
    "            transform=None),shuffle=False,batch_size=batch)\n",
    "\n",
    "        loader_val = DataLoader(CustomDataset(self.x_val, self.y_val,device = self.device,transform=None),\\\n",
    "            shuffle=False,batch_size=batch)  \n",
    "\n",
    "        sum_error = torch.nn.MSELoss(reduction='sum')       \n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            F_curr = 0.\n",
    "\n",
    "            for batch_idx in list(loader_tr.batch_sampler):\n",
    "            \n",
    "                inputs, targets, _ = loader_tr.dataset[batch_idx]\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                scores = self.model(inputs)\n",
    "                #print(self.criterion(scores, targets).item())\n",
    "\n",
    "                F_curr += sum_error(scores, targets).item() \n",
    "\n",
    "            #F_curr /= len(loader_tr.batch_sampler)\n",
    "            #print(F_curr,end=\",\")\n",
    "\n",
    "            l = [torch.flatten(p) for p in self.model.parameters()]\n",
    "            flatt = torch.cat(l)\n",
    "            l2_reg = torch.sum(flatt[:-1]*flatt[:-1])\n",
    "\n",
    "            valloss = 0.\n",
    "            for batch_idx in list(loader_val.batch_sampler):\n",
    "            \n",
    "                inputs, targets = loader_val.dataset[batch_idx]\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                scores = self.model(inputs)\n",
    "                valloss += self.criterion(scores, targets).item() \n",
    "            \n",
    "            constraint = valloss/len(loader_val.batch_sampler) - self.delta\n",
    "            multiplier = alphas*constraint #torch.dot(alphas,constraint)\n",
    "\n",
    "            F_curr += (self.lam*l2_reg*len(curr_subset) + multiplier).item()\n",
    "\n",
    "        val_mul = multiplier.item()\n",
    "        \n",
    "        #print(self.lam*l2_reg*len(curr_subset), multiplier)\n",
    "        #print(F_curr)\n",
    "\n",
    "        main_optimizer = torch.optim.Adam([{'params': self.model.parameters()}], lr=self.lr)\n",
    "        #dual_optimizer = torch.optim.Adam([{'params': alphas}], lr=self.lr)\n",
    "\n",
    "        l = [torch.flatten(p) for p in self.model.state_dict().values()]\n",
    "        flat = torch.cat(l).detach()\n",
    "\n",
    "        loader_tr = DataLoader(CustomDataset_WithId(self.x_trn[curr_subset], self.y_trn[curr_subset],\\\n",
    "            transform=None),shuffle=False,batch_size=self.batch_size)\n",
    "\n",
    "        #ele_delta = self.delta.repeat(min(self.batch_size,self.y_trn[curr_subset].shape[0])).to(self.device)\n",
    "\n",
    "        beta1,beta2 = main_optimizer.param_groups[0]['betas']\n",
    "        #main_optimizer.param_groups[0]['eps']\n",
    "\n",
    "        rem_len = (len(curr_subset)-1)\n",
    "\n",
    "        b_idxs = 0\n",
    "\n",
    "        device_new = self.device #\"cuda:2\" #self.device #\n",
    "\n",
    "        for batch_idx in list(loader_tr.batch_sampler):\n",
    "\n",
    "            inputs, targets, _ = loader_tr.dataset[batch_idx]\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "            ele_delta = self.delta.repeat(targets.shape[0]).to(self.device)\n",
    "        \n",
    "            weights = flat.repeat(targets.shape[0], 1)\n",
    "            ele_alphas = alphas.detach().repeat(targets.shape[0]).to(self.device)\n",
    "\n",
    "            exp_avg_w = w_exp_avg.repeat(targets.shape[0], 1)#torch.zeros_like(weights)\n",
    "            exp_avg_sq_w = w_exp_avg_sq.repeat(targets.shape[0], 1) #torch.zeros_like(weights)\n",
    "\n",
    "            exp_avg_a = a_exp_avg.repeat(targets.shape[0])#torch.zeros_like(ele_alphas)\n",
    "            exp_avg_sq_a = a_exp_avg_sq.repeat(targets.shape[0]) #torch.zeros_like(ele_alphas)\n",
    "\n",
    "            exten_inp = torch.cat((inputs,torch.ones(inputs.shape[0],device=self.device).view(-1,1)),dim=1)\n",
    "\n",
    "            bias_correction1 = beta1**step#1.0 \n",
    "            bias_correction2 = beta2**step#1.0 \n",
    "\n",
    "            for i in range(p_epoch):\n",
    "\n",
    "                fin_val_loss_g = torch.zeros_like(weights).to(device_new)\n",
    "                #val_losses = torch.zeros_like(ele_delta).to(device_new)\n",
    "                for batch_idx_val in list(loader_val.batch_sampler):\n",
    "                    \n",
    "                    inputs_val, targets_val = loader_val.dataset[batch_idx_val]\n",
    "                    inputs_val, targets_val = inputs_val.to(self.device), targets_val.to(self.device)\n",
    "\n",
    "                    exten_val = torch.cat((inputs_val,torch.ones(inputs_val.shape[0],device=self.device)\\\n",
    "                        .view(-1,1)),dim=1).to(device_new)\n",
    "                    #print(exten_val.shape)\n",
    "\n",
    "                    exten_val_y = targets_val.view(-1,1).repeat(1,min(self.batch_size,\\\n",
    "                        targets.shape[0])).to(device_new)\n",
    "                    #print(exten_val_y[0])\n",
    "                \n",
    "                    val_loss_p = 2*(torch.matmul(exten_val,torch.transpose(weights, 0, 1).to(device_new))\\\n",
    "                         - exten_val_y) \n",
    "                    #val_losses += torch.mean(val_loss_p*val_loss_p,dim=0)\n",
    "                    #val_loss_g = torch.unsqueeze(val_loss_p, dim=2).repeat(1,1,flat.shape[0])\n",
    "                    #print(val_loss_g[0][0])\n",
    "\n",
    "                    #mod_val = torch.unsqueeze(exten_val, dim=1).repeat(1,targets.shape[0],1)\n",
    "                    #print(mod_val[0])\n",
    "                    fin_val_loss_g += torch.mean(val_loss_p[:,:,None]*exten_val[:,None,:],dim=0)\n",
    "\n",
    "                    del exten_val,exten_val_y,val_loss_p,inputs_val, targets_val #mod_val,val_loss_g,\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                fin_val_loss_g /= len(loader_val.batch_sampler)\n",
    "                fin_val_loss_g = fin_val_loss_g.to(self.device)\n",
    "\n",
    "                sum_fin_trn_loss_g = torch.zeros_like(weights).to(device_new)\n",
    "                for batch_idx_trn in list(loader_tr.batch_sampler):\n",
    "                    \n",
    "                    inputs_trn, targets_trn,_ = loader_tr.dataset[batch_idx_trn]\n",
    "                    inputs_trn, targets_trn = inputs_trn.to(self.device), targets_trn.to(self.device)\n",
    "\n",
    "                    exten_trn = torch.cat((inputs_trn,torch.ones(inputs_trn.shape[0]\\\n",
    "                        ,device=self.device).view(-1,1)),dim=1).to(device_new)\n",
    "                    exten_trn_y = targets_trn.view(-1,1).repeat(1,min(self.batch_size,\\\n",
    "                        targets.shape[0])).to(device_new)\n",
    "                    #print(exten_val_y[0])\n",
    "                \n",
    "                    sum_trn_loss_p = 2*(torch.matmul(exten_trn,torch.transpose(weights, 0, 1)\\\n",
    "                        .to(device_new)) - exten_trn_y)\n",
    "                    #sum_trn_loss_g = torch.unsqueeze(trn_loss_p, dim=2).repeat(1,1,flat.shape[0])\n",
    "\n",
    "                    #mod_trn = torch.unsqueeze(exten_trn, dim=1).repeat(1,targets.shape[0],1)\n",
    "                    sum_fin_trn_loss_g += torch.sum(sum_trn_loss_p[:,:,None]*exten_trn[:,None,:],dim=0)\n",
    "\n",
    "                    #print(sum_fin_trn_loss_g.shape)\n",
    "\n",
    "                    del exten_trn,exten_trn_y,sum_trn_loss_p,inputs_trn, targets_trn #mod_trn,sum_trn_loss_g,\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                #fin_trn_loss_g /= len(loader_tr.batch_sampler)\n",
    "                sum_fin_trn_loss_g = sum_fin_trn_loss_g.to(self.device)\n",
    "\n",
    "                trn_loss_g = torch.sum(exten_inp*weights,dim=1) - targets\n",
    "                fin_trn_loss_g = exten_inp*2*trn_loss_g[:,None]\n",
    "\n",
    "                fin_trn_loss_g = (sum_fin_trn_loss_g - fin_trn_loss_g)/rem_len\n",
    "\n",
    "                weight_grad = fin_trn_loss_g+ 2*rem_len*\\\n",
    "                    torch.cat((weights[:,:-1], torch.zeros((weights.shape[0],1),device=self.device)),dim=1)+\\\n",
    "                        fin_val_loss_g*ele_alphas[:,None]\n",
    "\n",
    "                #print(weight_grad[0])\n",
    "\n",
    "                exp_avg_w.mul_(beta1).add_(1.0 - beta1, weight_grad)\n",
    "                exp_avg_sq_w.mul_(beta2).addcmul_(1.0 - beta2, weight_grad, weight_grad)\n",
    "                denom = exp_avg_sq_w.sqrt().add_(main_optimizer.param_groups[0]['eps'])\n",
    "\n",
    "                bias_correction1 *= beta1\n",
    "                bias_correction2 *= beta2\n",
    "                step_size = (self.lr)* math.sqrt(1.0-bias_correction2) / (1.0-bias_correction1)\n",
    "                weights.addcdiv_(-step_size, exp_avg_w, denom)\n",
    "                \n",
    "                #weights = weights - self.lr*(weight_grad)\n",
    "\n",
    "                '''print(self.lr)\n",
    "                print((fin_trn_loss_g+ 2*self.lam*weights +fin_val_loss_g*ele_alphas[:,None])[0])'''\n",
    "\n",
    "                #print(weights[0])\n",
    "\n",
    "                val_losses = torch.zeros_like(ele_delta).to(device_new)\n",
    "                for batch_idx_val in list(loader_val.batch_sampler):\n",
    "                    \n",
    "                    inputs_val, targets_val = loader_val.dataset[batch_idx_val]\n",
    "                    inputs_val, targets_val = inputs_val.to(self.device), targets_val.to(self.device)\n",
    "\n",
    "                    exten_val = torch.cat((inputs_val,torch.ones(inputs_val.shape[0],device=self.device)\\\n",
    "                        .view(-1,1)),dim=1).to(device_new)\n",
    "                    #print(exten_val.shape)\n",
    "\n",
    "                    exten_val_y = targets_val.view(-1,1).repeat(1,min(self.batch_size,\\\n",
    "                        targets.shape[0])).to(device_new)\n",
    "                    #print(exten_val_y[0])\n",
    "                \n",
    "                    val_loss_p = torch.matmul(exten_val,torch.transpose(weights, 0, 1).to(device_new))\\\n",
    "                         - exten_val_y #\n",
    "                    val_losses += torch.mean(val_loss_p*val_loss_p,dim=0)\n",
    "\n",
    "                    del exten_val,exten_val_y,val_loss_p,inputs_val, targets_val\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                val_losses = val_losses.to(self.device)\n",
    "\n",
    "                alpha_grad = val_losses/len(loader_val.batch_sampler)-ele_delta\n",
    "\n",
    "                exp_avg_a.mul_(beta1).add_(1.0 - beta1, alpha_grad)\n",
    "                exp_avg_sq_a.mul_(beta2).addcmul_(1.0 - beta2, alpha_grad, alpha_grad)\n",
    "                denom = exp_avg_sq_a.sqrt().add_(main_optimizer.param_groups[0]['eps'])\n",
    "                ele_alphas.addcdiv_(step_size, exp_avg_a, denom)\n",
    "                ele_alphas[ele_alphas < 0] = 0\n",
    "                #print(ele_alphas[0])\n",
    "\n",
    "                #ele_alphas = ele_alphas + self.lr*(torch.mean(val_loss_p*val_loss_p,dim=0)-ele_delta)\n",
    "\n",
    "            val_losses = 0.\n",
    "            for batch_idx_val in list(loader_val.batch_sampler):\n",
    "                    \n",
    "                inputs_val, targets_val = loader_val.dataset[batch_idx_val]\n",
    "                inputs_val, targets_val = inputs_val.to(self.device), targets_val.to(self.device)\n",
    "\n",
    "                exten_val = torch.cat((inputs_val,torch.ones(inputs_val.shape[0],device=self.device).view(-1,1)),dim=1)\n",
    "                exten_val_y = targets_val.view(-1,1).repeat(1,targets.shape[0])\n",
    "                \n",
    "                val_loss = torch.matmul(exten_val,torch.transpose(weights, 0, 1)) - exten_val_y\n",
    "\n",
    "                val_losses+= torch.mean(val_loss*val_loss,dim=0)\n",
    "            \n",
    "            reg = torch.sum(weights[:,:-1]*weights[:,:-1],dim=1)\n",
    "\n",
    "            trn_losses = 0.\n",
    "            for batch_idx_trn in list(loader_tr.batch_sampler):\n",
    "                    \n",
    "                inputs_trn, targets_trn,_ = loader_tr.dataset[batch_idx_trn]\n",
    "                inputs_trn, targets_trn = inputs_trn.to(self.device), targets_trn.to(self.device)\n",
    "\n",
    "                exten_trn = torch.cat((inputs_trn,torch.ones(inputs_trn.shape[0],device=self.device).view(-1,1)),dim=1)\n",
    "                exten_trn_y = targets_trn.view(-1,1).repeat(1,min(self.batch_size,targets.shape[0]))\n",
    "                #print(exten_val_y[0])\n",
    "            \n",
    "                trn_loss = torch.matmul(exten_trn,torch.transpose(weights, 0, 1)) - exten_trn_y\n",
    "                \n",
    "                trn_losses+= torch.sum(trn_loss*trn_loss,dim=0)\n",
    "\n",
    "            trn_loss_ind = torch.sum(exten_inp*weights,dim=1) - targets\n",
    "\n",
    "            trn_losses -= trn_loss_ind*trn_loss_ind\n",
    "\n",
    "            abs_value = F_curr - (trn_losses + self.lam*reg*rem_len \\\n",
    "                + (val_losses/len(loader_val.batch_sampler)-ele_delta)*ele_alphas) \n",
    "\n",
    "            neg_ind = ((abs_value ) < 0).nonzero().view(-1)\n",
    "\n",
    "            abs_value [neg_ind] = torch.max(self.F_values)\n",
    "\n",
    "            m_values[torch.tensor(curr_subset, dtype = torch.long)[b_idxs*self.batch_size:(b_idxs+1)*self.batch_size]]\\\n",
    "                 = abs_value\n",
    "\n",
    "            b_idxs +=1\n",
    "\n",
    "        values,indices =m_values.topk(budget,largest=False)\n",
    "\n",
    "        return list(indices.cpu().numpy())\n",
    "\n",
    "class FindSubset_Vect_TrnLoss(object):\n",
    "    def __init__(self, x_trn, y_trn, x_val, y_val,model,loss,device,delta,lr,lam,batch):\n",
    "        \n",
    "        self.x_trn = x_trn\n",
    "        self.y_trn = y_trn\n",
    "        #self.trn_batch = trn_batch\n",
    "\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = loss \n",
    "        self.device = device\n",
    "\n",
    "        self.delta = delta\n",
    "        self.lr = lr\n",
    "        self.lam = lam\n",
    "        #self.optimizer = optimizer\n",
    "        self.batch_size = batch\n",
    "\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "        np.random.seed(42)\n",
    "\n",
    "\n",
    "    def precompute(self,f_pi_epoch,p_epoch,alphas):\n",
    "\n",
    "        main_optimizer = torch.optim.Adam([\n",
    "                {'params': self.model.parameters()}], lr=self.lr)\n",
    "                \n",
    "        dual_optimizer = torch.optim.Adam([{'params': alphas}], lr=self.lr)\n",
    "\n",
    "        print(\"starting Pre compute\")\n",
    "        #alphas = torch.rand_like(self.delta,requires_grad=True) \n",
    "\n",
    "        #print(alphas)\n",
    "\n",
    "        #scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[10,20,40,100],\\\n",
    "        #    gamma=0.5) #[e*2 for e in change]\n",
    "\n",
    "        #alphas.requires_grad = False\n",
    "        loader_val = DataLoader(CustomDataset(self.x_val, self.y_val,transform=None),\\\n",
    "            shuffle=False,batch_size=self.batch_size)\n",
    "            \n",
    "\n",
    "        #Compute F_phi\n",
    "        #for i in range(f_pi_epoch):\n",
    "\n",
    "        prev_loss = 1000\n",
    "        stop_count = 0\n",
    "        i=0\n",
    "        \n",
    "        while(True):\n",
    "            \n",
    "            main_optimizer.zero_grad()\n",
    "            \n",
    "            '''l2_reg = 0\n",
    "            for param in self.model.parameters():\n",
    "                l2_reg += torch.norm(param)'''\n",
    "\n",
    "            #l = [torch.flatten(p) for p in main_model.parameters()]\n",
    "            #flat = torch.cat(l)\n",
    "            #l2_reg = torch.sum(flat*flat)\n",
    "            \n",
    "            constraint = 0. \n",
    "            for batch_idx in list(loader_val.batch_sampler):\n",
    "                    \n",
    "                inputs, targets = loader_val.dataset[batch_idx]\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "                val_out = self.model(inputs)\n",
    "                constraint += self.criterion(val_out, targets)\n",
    "                \n",
    "            constraint /= len(loader_val.batch_sampler)\n",
    "            constraint = constraint - self.delta\n",
    "            multiplier = alphas*constraint #torch.dot(alphas,constraint)\n",
    "\n",
    "            loss = multiplier\n",
    "            self.F_phi = loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            main_optimizer.step()\n",
    "            #scheduler.step()\n",
    "            \n",
    "            '''for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            alphas.requires_grad = True'''\n",
    "\n",
    "            dual_optimizer.zero_grad()\n",
    "\n",
    "            constraint = 0.\n",
    "            for batch_idx in list(loader_val.batch_sampler):\n",
    "                    \n",
    "                inputs, targets = loader_val.dataset[batch_idx]\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "                val_out = self.model(inputs)\n",
    "                constraint += self.criterion(val_out, targets)\n",
    "            \n",
    "            constraint /= len(loader_val.batch_sampler)\n",
    "            constraint = constraint - self.delta\n",
    "            multiplier = -1.0*alphas*constraint #torch.dot(-1.0*alphas,constraint)\n",
    "            \n",
    "            multiplier.backward()\n",
    "            \n",
    "            dual_optimizer.step()\n",
    "\n",
    "            alphas.requires_grad = False\n",
    "            alphas.clamp_(min=0.0)\n",
    "            alphas.requires_grad = True\n",
    "            #print(alphas)\n",
    "\n",
    "            '''for param in self.model.parameters():\n",
    "                param.requires_grad = True'''\n",
    "\n",
    "            if loss.item() <= 0.:\n",
    "                break\n",
    "\n",
    "            #if i>= f_pi_epoch:\n",
    "            #    break\n",
    "\n",
    "            if abs(prev_loss - loss.item()) <= 1e-3 and stop_count >= 5:\n",
    "                break \n",
    "            elif abs(prev_loss - loss.item()) <= 1e-3:\n",
    "                stop_count += 1\n",
    "            else:\n",
    "                stop_count = 0\n",
    "\n",
    "            prev_loss = loss.item()\n",
    "            i+=1\n",
    "\n",
    "            #if i % 50 == 0:\n",
    "            #    print(loss.item(),alphas,constraint)\n",
    "\n",
    "        print(\"Finishing F phi\")\n",
    "\n",
    "        if loss.item() <= 0.:\n",
    "            alphas = torch.zeros_like(alphas)\n",
    "\n",
    "        #print(loss.item())\n",
    "\n",
    "        l = [torch.flatten(p) for p in self.model.state_dict().values()]\n",
    "        flat = torch.cat(l).detach().clone()\n",
    "        \n",
    "        self.F_values = torch.zeros(len(self.x_trn),device=self.device)\n",
    "\n",
    "        device_new = self.device #\"cuda:2\"#self.device #\n",
    "        beta1,beta2 = main_optimizer.param_groups[0]['betas']\n",
    "        #main_optimizer.param_groups[0]['eps']\n",
    "\n",
    "        loader_tr = DataLoader(CustomDataset_WithId(self.x_trn, self.y_trn,\\\n",
    "            transform=None), device = self.device, shuffle=False,batch_size=self.batch_size*20)\n",
    "\n",
    "        loader_val = DataLoader(CustomDataset(self.x_val, self.y_val,device = self.device,transform=None),\\\n",
    "            shuffle=False,batch_size=self.batch_size*20)\n",
    "        \n",
    "        for batch_idx in list(loader_tr.batch_sampler):\n",
    "\n",
    "            inputs, targets, idxs = loader_tr.dataset[batch_idx]\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "            ele_delta = self.delta.repeat(targets.shape[0]).to(self.device)\n",
    "        \n",
    "            weights = flat.view(1,-1).repeat(targets.shape[0], 1)\n",
    "            ele_alphas = alphas.detach().repeat(targets.shape[0]).to(self.device)\n",
    "            #print(weights.shape)\n",
    "\n",
    "            exp_avg_w = torch.zeros_like(weights)\n",
    "            exp_avg_sq_w = torch.zeros_like(weights)\n",
    "\n",
    "            #exp_avg_a = torch.zeros_like(ele_alphas)\n",
    "            #exp_avg_sq_a = torch.zeros_like(ele_alphas)\n",
    "\n",
    "            exten_inp = torch.cat((inputs,torch.ones(inputs.shape[0],device=self.device).view(-1,1))\\\n",
    "                ,dim=1)\n",
    "\n",
    "            bias_correction1 = 1.0 \n",
    "            bias_correction2 = 1.0 \n",
    "\n",
    "            for i in range(p_epoch):\n",
    "\n",
    "                trn_loss_g = torch.sum(exten_inp*weights,dim=1) - targets\n",
    "                fin_trn_loss_g = exten_inp*2*trn_loss_g[:,None]\n",
    "\n",
    "                #no_bias = weights.clone()\n",
    "                #no_bias[-1,:] = torch.zeros(weights.shape[0])\n",
    "                \n",
    "                weight_grad = fin_trn_loss_g+ 2*self.lam*\\\n",
    "                    torch.cat((weights[:,:-1], torch.zeros((weights.shape[0],1),device=self.device)),dim=1)\n",
    "                #+fin_val_loss_g*ele_alphas[:,None]\n",
    "\n",
    "                exp_avg_w.mul_(beta1).add_(1.0 - beta1, weight_grad)\n",
    "                exp_avg_sq_w.mul_(beta2).addcmul_(1.0 - beta2, weight_grad, weight_grad)\n",
    "                denom = exp_avg_sq_w.sqrt().add_(main_optimizer.param_groups[0]['eps'])\n",
    "\n",
    "                bias_correction1 *= beta1\n",
    "                bias_correction2 *= beta2\n",
    "                step_size = (self.lr) * math.sqrt(1.0-bias_correction2) / (1.0-bias_correction1)\n",
    "                weights.addcdiv_(-step_size, exp_avg_w, denom)\n",
    "                \n",
    "            val_losses = 0.\n",
    "            for batch_idx_val in list(loader_val.batch_sampler):\n",
    "                    \n",
    "                inputs_val, targets_val = loader_val.dataset[batch_idx_val]\n",
    "                inputs_val, targets_val = inputs_val.to(self.device), targets_val.to(self.device)\n",
    "\n",
    "                exten_val = torch.cat((inputs_val,torch.ones(inputs_val.shape[0],device=self.device).view(-1,1)),dim=1)\n",
    "                #exten_val_y = targets_val.view(-1,1).repeat(1,min(self.batch_size*20,targets.shape[0]))\n",
    "                \n",
    "                exten_val_y = torch.mean(targets_val).repeat(min(self.batch_size*20,targets.shape[0]))\n",
    "\n",
    "                #val_loss = torch.matmul(torch.mean(exten_val,dim=0),weights.T).view(-1) - exten_val_y \n",
    "                #torch.transpose(weights, 0, 1)\n",
    "\n",
    "                val_loss = torch.sum(weights*torch.mean(exten_val,dim=0),dim=1) - exten_val_y\n",
    "\n",
    "                val_losses+= val_loss*val_loss #torch.mean(val_loss*val_loss,dim=0)\n",
    "            \n",
    "            reg = torch.sum(weights[:,:-1]*weights[:,:-1],dim=1)\n",
    "            trn_loss = torch.sum(exten_inp*weights,dim=1) - targets\n",
    "\n",
    "            #print(torch.sum(exten_inp*weights,dim=1)[0])\n",
    "            #print((trn_loss*trn_loss)[0],self.lam*reg[0],\\\n",
    "            #    ((torch.mean(val_loss*val_loss,dim=0)-ele_delta)*ele_alphas)[0])\n",
    "\n",
    "            self.F_values[idxs] = trn_loss*trn_loss+ self.lam*reg +torch.max(torch.zeros_like(ele_alphas),\\\n",
    "                (val_losses/len(loader_val.batch_sampler)-ele_delta)*ele_alphas)\n",
    "\n",
    "        #print(self.F_values[:10])\n",
    "\n",
    "        #self.F_values = self.F_values - max(loss.item(),0.) \n",
    "\n",
    "        print(\"Finishing Element wise F\")\n",
    "\n",
    "\n",
    "    def return_subset(self,theta_init,p_epoch,curr_subset,budget,batch,\\\n",
    "        step,w_exp_avg,w_exp_avg_sq):#,alphas,a_exp_avg,a_exp_avg_sq):\n",
    "\n",
    "        m_values = self.F_values.detach().clone() #torch.zeros(len(self.x_trn))\n",
    "        \n",
    "        self.model.load_state_dict(theta_init)\n",
    "\n",
    "        #print(theta_init)\n",
    "        #print(curr_subset)\n",
    "\n",
    "        '''main_optimizer = torch.optim.Adam([\n",
    "                {'params': self.model.parameters()}], lr=self.lr)\n",
    "                \n",
    "        dual_optimizer = torch.optim.Adam([{'params': alphas}], lr=self.lr)'''\n",
    "\n",
    "        loader_tr = DataLoader(CustomDataset_WithId(self.x_trn[curr_subset], self.y_trn[curr_subset],\\\n",
    "            transform=None),shuffle=False,batch_size=batch)\n",
    "\n",
    "        #loader_val = DataLoader(CustomDataset(self.x_val, self.y_val,transform=None),\\\n",
    "        #    shuffle=False,batch_size=batch)  \n",
    "\n",
    "        sum_error = torch.nn.MSELoss(reduction='sum')       \n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            F_curr = 0.\n",
    "\n",
    "            for batch_idx in list(loader_tr.batch_sampler):\n",
    "            \n",
    "                inputs, targets, _ = loader_tr.dataset[batch_idx]\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                scores = self.model(inputs)\n",
    "                #print(self.criterion(scores, targets).item())\n",
    "\n",
    "                F_curr += sum_error(scores, targets).item() \n",
    "\n",
    "            #F_curr /= len(loader_tr.batch_sampler)\n",
    "            #print(F_curr,end=\",\")\n",
    "\n",
    "            l = [torch.flatten(p) for p in self.model.parameters()]\n",
    "            flatt = torch.cat(l)\n",
    "            l2_reg = torch.sum(flatt[:-1]*flatt[:-1])\n",
    "\n",
    "            F_curr += (self.lam*l2_reg*len(curr_subset)).item() #+ multiplier).item()\n",
    "\n",
    "        #val_mul = multiplier.item()\n",
    "        \n",
    "        #print(self.lam*l2_reg*len(curr_subset))#, multiplier)\n",
    "        #print(F_curr)\n",
    "\n",
    "        main_optimizer = torch.optim.Adam([{'params': self.model.parameters()}], lr=self.lr)\n",
    "        #dual_optimizer = torch.optim.Adam([{'params': alphas}], lr=self.lr)\n",
    "\n",
    "        l = [torch.flatten(p) for p in self.model.state_dict().values()]\n",
    "        flat = torch.cat(l).detach()\n",
    "\n",
    "        loader_tr = DataLoader(CustomDataset_WithId(self.x_trn[curr_subset], self.y_trn[curr_subset],\\\n",
    "            transform=None),shuffle=False,batch_size=self.batch_size)\n",
    "\n",
    "        #ele_delta = self.delta.repeat(min(self.batch_size,self.y_trn[curr_subset].shape[0])).to(self.device)\n",
    "\n",
    "        beta1,beta2 = main_optimizer.param_groups[0]['betas']\n",
    "        #main_optimizer.param_groups[0]['eps']\n",
    "\n",
    "        rem_len = (len(curr_subset)-1)\n",
    "\n",
    "        b_idxs = 0\n",
    "\n",
    "        device_new = self.device #\"cuda:2\" #self.device #\n",
    "\n",
    "        for batch_idx in list(loader_tr.batch_sampler):\n",
    "\n",
    "            inputs, targets, _ = loader_tr.dataset[batch_idx]\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "            #ele_delta = self.delta.repeat(targets.shape[0]).to(self.device)\n",
    "        \n",
    "            weights = flat.repeat(targets.shape[0], 1)\n",
    "            #ele_alphas = alphas.detach().repeat(targets.shape[0]).to(self.device)\n",
    "\n",
    "            exp_avg_w = w_exp_avg.repeat(targets.shape[0], 1)#torch.zeros_like(weights)\n",
    "            exp_avg_sq_w = w_exp_avg_sq.repeat(targets.shape[0], 1) #torch.zeros_like(weights)\n",
    "\n",
    "            #exp_avg_a = a_exp_avg.repeat(targets.shape[0])#torch.zeros_like(ele_alphas)\n",
    "            #exp_avg_sq_a = a_exp_avg_sq.repeat(targets.shape[0]) #torch.zeros_like(ele_alphas)\n",
    "\n",
    "            exten_inp = torch.cat((inputs,torch.ones(inputs.shape[0],device=self.device).view(-1,1)),dim=1)\n",
    "\n",
    "            bias_correction1 = beta1**step#1.0 \n",
    "            bias_correction2 = beta2**step#1.0 \n",
    "\n",
    "            for i in range(p_epoch):\n",
    "\n",
    "                sum_fin_trn_loss_g = torch.zeros_like(weights).to(device_new)\n",
    "                for batch_idx_trn in list(loader_tr.batch_sampler):\n",
    "                    \n",
    "                    inputs_trn, targets_trn,_ = loader_tr.dataset[batch_idx_trn]\n",
    "                    inputs_trn, targets_trn = inputs_trn.to(self.device), targets_trn.to(self.device)\n",
    "\n",
    "                    exten_trn = torch.cat((inputs_trn,torch.ones(inputs_trn.shape[0]\\\n",
    "                        ,device=self.device).view(-1,1)),dim=1).to(device_new)\n",
    "                    exten_trn_y = targets_trn.view(-1,1).repeat(1,min(self.batch_size,\\\n",
    "                        targets.shape[0])).to(device_new)\n",
    "                    #print(exten_val_y[0])\n",
    "                \n",
    "                    sum_trn_loss_p = 2*(torch.matmul(exten_trn,torch.transpose(weights, 0, 1)\\\n",
    "                        .to(device_new)) - exten_trn_y)\n",
    "                    #sum_trn_loss_g = torch.unsqueeze(trn_loss_p, dim=2).repeat(1,1,flat.shape[0])\n",
    "\n",
    "                    #mod_trn = torch.unsqueeze(exten_trn, dim=1).repeat(1,targets.shape[0],1)\n",
    "                    sum_fin_trn_loss_g += torch.sum(sum_trn_loss_p[:,:,None]*exten_trn[:,None,:],dim=0)\n",
    "\n",
    "                    #print(sum_fin_trn_loss_g.shape)\n",
    "\n",
    "                    del exten_trn,exten_trn_y,sum_trn_loss_p,inputs_trn, targets_trn #mod_trn,sum_trn_loss_g,\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                #fin_trn_loss_g /= len(loader_tr.batch_sampler)\n",
    "                sum_fin_trn_loss_g = sum_fin_trn_loss_g.to(self.device)\n",
    "\n",
    "                trn_loss_g = torch.sum(exten_inp*weights,dim=1) - targets\n",
    "                fin_trn_loss_g = exten_inp*2*trn_loss_g[:,None]\n",
    "\n",
    "                fin_trn_loss_g = (sum_fin_trn_loss_g - fin_trn_loss_g)/rem_len\n",
    "\n",
    "                weight_grad = fin_trn_loss_g+ 2*rem_len*\\\n",
    "                    torch.cat((weights[:,:-1], torch.zeros((weights.shape[0],1),device=self.device)),dim=1)#+\\\n",
    "                #fin_val_loss_g*ele_alphas[:,None]\n",
    "\n",
    "                #print(weight_grad[0])\n",
    "\n",
    "                exp_avg_w.mul_(beta1).add_(1.0 - beta1, weight_grad)\n",
    "                exp_avg_sq_w.mul_(beta2).addcmul_(1.0 - beta2, weight_grad, weight_grad)\n",
    "                denom = exp_avg_sq_w.sqrt().add_(main_optimizer.param_groups[0]['eps'])\n",
    "\n",
    "                bias_correction1 *= beta1\n",
    "                bias_correction2 *= beta2\n",
    "                step_size = (self.lr)* math.sqrt(1.0-bias_correction2) / (1.0-bias_correction1)\n",
    "                weights.addcdiv_(-step_size, exp_avg_w, denom)\n",
    "            \n",
    "            reg = torch.sum(weights[:,:-1]*weights[:,:-1],dim=1)\n",
    "\n",
    "            trn_losses = 0.\n",
    "            for batch_idx_trn in list(loader_tr.batch_sampler):\n",
    "                    \n",
    "                inputs_trn, targets_trn,_ = loader_tr.dataset[batch_idx_trn]\n",
    "                inputs_trn, targets_trn = inputs_trn.to(self.device), targets_trn.to(self.device)\n",
    "\n",
    "                exten_trn = torch.cat((inputs_trn,torch.ones(inputs_trn.shape[0],device=self.device).view(-1,1)),dim=1)\n",
    "                exten_trn_y = targets_trn.view(-1,1).repeat(1,min(self.batch_size,targets.shape[0]))\n",
    "                #print(exten_val_y[0])\n",
    "            \n",
    "                trn_loss = torch.matmul(exten_trn,torch.transpose(weights, 0, 1)) - exten_trn_y\n",
    "                \n",
    "                trn_losses+= torch.sum(trn_loss*trn_loss,dim=0)\n",
    "\n",
    "            trn_loss_ind = torch.sum(exten_inp*weights,dim=1) - targets\n",
    "\n",
    "            trn_losses -= trn_loss_ind*trn_loss_ind\n",
    "            \n",
    "            abs_value = F_curr - (trn_losses + self.lam*reg*rem_len) #\\\n",
    "            \n",
    "            neg_ind = ((abs_value ) < 0).nonzero().view(-1)\n",
    "\n",
    "            abs_value [neg_ind] = torch.max(self.F_values)\n",
    "\n",
    "            m_values[torch.tensor(curr_subset)[b_idxs*self.batch_size:(b_idxs+1)*self.batch_size]]\\\n",
    "                 = abs_value\n",
    "\n",
    "            b_idxs +=1\n",
    "\n",
    "        values,indices =m_values.topk(budget,largest=False)\n",
    "\n",
    "        return list(indices.cpu().numpy())\n",
    "\n",
    "\n",
    "class SetFunctionFacLoc(object):\n",
    "\n",
    "    def __init__(self, device, train_full_loader):#, valid_loader):\n",
    "        \n",
    "        self.train_loader = train_full_loader      \n",
    "        self.device = device #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\n",
    "    def distance(self,x, y, exp = 2):\n",
    "\n",
    "      n = x.size(0)\n",
    "      m = y.size(0)\n",
    "      d = x.size(1)\n",
    "\n",
    "      x = x.unsqueeze(1).expand(n, m, d)\n",
    "      y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "      dist = torch.pow(x - y, exp).sum(2) \n",
    "      return dist \n",
    "\n",
    "    def compute_score(self):\n",
    "      self.N = 0\n",
    "      g_is = []#self.train_loader\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for i, data_i in  enumerate(self.train_loader, 0):\n",
    "          inputs_i, target_i = data_i\n",
    "          inputs_i = inputs_i.to(self.device) #, target_i.to(self.device)\n",
    "          self.N += inputs_i.size()[0]\n",
    "          g_is.append(inputs_i)\n",
    "        \n",
    "        self.sim_mat = torch.zeros([self.N, self.N],dtype=torch.float32)\n",
    "\n",
    "        first_i = True\n",
    "\n",
    "        for i, g_i in enumerate(g_is, 0):\n",
    "\n",
    "          if first_i:\n",
    "            size_b = g_i.size(0)\n",
    "            first_i = False\n",
    "\n",
    "          for j, g_j in enumerate(g_is, 0):\n",
    "            self.sim_mat[i*size_b: i*size_b + g_i.size(0), j*size_b: j*size_b + g_j.size(0)] = self.distance(g_i, g_j)\n",
    "      self.const = torch.max(self.sim_mat).item()\n",
    "      self.sim_mat = self.const - self.sim_mat\n",
    "      #self.sim_mat = self.sim_mat.to(self.device)\n",
    "      dist = self.sim_mat.sum(1)\n",
    "      bestId = torch.argmax(dist).item()\n",
    "      self.max_sim = self.sim_mat[bestId].to(self.device)\n",
    "      return bestId\n",
    "\n",
    "\n",
    "    def lazy_greedy_max(self, budget,logfile):\n",
    "      \n",
    "      #starting = time.process_time() \n",
    "      id_first = self.compute_score()\n",
    "      self.gains = PriorityQueue()\n",
    "      for i in range(self.N):\n",
    "        if i == id_first :\n",
    "          continue\n",
    "        curr_gain = (torch.max(self.max_sim ,self.sim_mat[i].to(self.device)) - self.max_sim).sum()\n",
    "        self.gains.put((-curr_gain.item(),i))\n",
    "\n",
    "      numSelected = 2\n",
    "      second = self.gains.get()\n",
    "      greedyList = [id_first, second[1]]\n",
    "      self.max_sim = torch.max(self.max_sim,self.sim_mat[second[1]].to(self.device))\n",
    "\n",
    "      #ending = time.process_time()\n",
    "      #print(\"Kernel computation time \",ending-starting, file=logfile)\n",
    "\n",
    "      starting = time.process_time()\n",
    "      while(numSelected < budget):\n",
    "\n",
    "          if self.gains.empty():\n",
    "            break\n",
    "\n",
    "          elif self.gains.qsize() == 1:\n",
    "            bestId = self.gains.get()[1]\n",
    "\n",
    "          else:\n",
    "   \n",
    "            bestGain = -np.inf\n",
    "            bestId = None\n",
    "            \n",
    "            while True:\n",
    "\n",
    "              first =  self.gains.get()\n",
    "\n",
    "              if bestId == first[1]: \n",
    "                break\n",
    "\n",
    "              curr_gain = (torch.max(self.max_sim, self.sim_mat[first[1]].to(self.device)) - self.max_sim).sum()\n",
    "              self.gains.put((-curr_gain.item(), first[1]))\n",
    "\n",
    "\n",
    "              if curr_gain.item() >= bestGain:\n",
    "                  \n",
    "                bestGain = curr_gain.item()\n",
    "                bestId = first[1]\n",
    "\n",
    "          greedyList.append(bestId)\n",
    "          numSelected += 1\n",
    "\n",
    "          self.max_sim = torch.max(self.max_sim,self.sim_mat[bestId].to(self.device))\n",
    "\n",
    "      #print()\n",
    "      #gamma = self.compute_gamma(greedyList)\n",
    "\n",
    "      #ending = time.process_time()\n",
    "      #print(\"Selectio time \",ending-starting, file=logfile)\n",
    "\n",
    "      return greedyList\n",
    "\n",
    "\n",
    "\n",
    "def process_time_series(datadir, past_length,col_name,name,save_data=True):\n",
    "\n",
    "    x_trn = []\n",
    "    y_trn = []\n",
    "\n",
    "    path = os.path.join(datadir, 'prices-split-adjusted.csv')\n",
    "\n",
    "    prices_split_adjusted = pd.read_csv(path,index_col='date', parse_dates=['date'])\n",
    "\n",
    "    #Removing data with less than 1500 entries\n",
    "    symbol = pd.DataFrame(prices_split_adjusted['symbol'].value_counts())\n",
    "    omit = symbol[symbol['symbol'] < 1500].index\n",
    "    prices_split_adjusted = prices_split_adjusted[~prices_split_adjusted['symbol'].isin(omit)]\n",
    "\n",
    "    symbol = pd.DataFrame(prices_split_adjusted['symbol'].value_counts())\n",
    "\n",
    "    for sym in symbol.index:\n",
    "\n",
    "        stock = prices_split_adjusted[prices_split_adjusted['symbol'] == sym]\n",
    "\n",
    "        data_len = len(stock.index)\n",
    "        \n",
    "        x_trn_slice = np.zeros((data_len-past_length,past_length))\n",
    "        y_trn_slice = np.zeros(data_len-past_length)\n",
    "        \n",
    "        for ind in range(data_len-past_length):\n",
    "            x_trn_slice[ind] = stock[col_name][stock.index[ind:ind+past_length]]\n",
    "            y_trn_slice[ind] = stock[col_name][stock.index[ind+past_length]]\n",
    "\n",
    "        x_trn.append(x_trn_slice)\n",
    "        y_trn.append(y_trn_slice)\n",
    "\n",
    "    finaL_x_trn = np.concatenate(x_trn, axis=0)\n",
    "    finaL_y_trn = np.concatenate(y_trn, axis=0)\n",
    "\n",
    "    \n",
    "    if save_data:\n",
    "        # Save the numpy files to the folder where they come from\n",
    "        data_np_path = os.path.join(datadir, name+'_'+str(past_length)+'.data.npy') \n",
    "        target_np_path = os.path.join(datadir, name+'_'+str(past_length)+'.label.npy') \n",
    "        np.save(data_np_path, finaL_x_trn)\n",
    "        np.save(target_np_path, finaL_y_trn)\n",
    "\n",
    "    return finaL_x_trn, finaL_y_trn\n",
    "\n",
    "\n",
    "\n",
    "def load_time_series_data (datadir, dset_name,past_length,clean=True):\n",
    "\n",
    "    if os.path.isfile(os.path.join(datadir, dset_name+'_'+str(past_length)+'.data.npy')) and \\\n",
    "            os.path.isfile(os.path.join(datadir, dset_name+'_'+str(past_length)+'.label.npy')):\n",
    "        \n",
    "        x_trn = np.load(os.path.join(datadir, dset_name+'_'+str(past_length)+'.data.npy'))\n",
    "        y_trn  = np.load(os.path.join(datadir, dset_name+'_'+str(past_length)+'.label.npy'))\n",
    "\n",
    "    else:\n",
    "    \n",
    "        if dset_name == \"NY_Stock_exchange_close\":\n",
    "            x_trn, y_trn  = process_time_series(datadir,past_length,'close',dset_name)\n",
    "\n",
    "        elif dset_name == \"NY_Stock_exchange_open\":\n",
    "            x_trn, y_trn  = process_time_series(datadir,past_length,'open',dset_name)\n",
    "\n",
    "        elif dset_name == \"NY_Stock_exchange_high\":\n",
    "            x_trn, y_trn  = process_time_series(datadir,past_length,'high',dset_name)\n",
    "\n",
    "        elif dset_name == \"NY_Stock_exchange_low\":\n",
    "            x_trn, y_trn  = process_time_series(datadir,past_length,'low',dset_name)\n",
    "\n",
    "    y_trn = y_trn #+ 1600\n",
    "    \n",
    "    x_trn, x_tst, y_trn, y_tst = train_test_split(x_trn, y_trn, test_size=0.1, random_state=42)\n",
    "    x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.005, random_state=42)\n",
    "    \n",
    "    if not clean:\n",
    "\n",
    "        noise_size = int(len(y_trn) * 0.5)\n",
    "        noise_indices = np.random.choice(np.arange(len(y_trn)), size=noise_size, replace=False)\n",
    "        \n",
    "        sigma = 40\n",
    "        y_trn[noise_indices] = y_trn[noise_indices] + np.random.normal(0, sigma, noise_size)\n",
    "    \n",
    "    sc = MinMaxScaler() #StandardScaler()\n",
    "    x_trn = sc.fit_transform(x_trn)\n",
    "    x_val = sc.transform(x_val)\n",
    "    x_tst = sc.transform(x_tst)\n",
    "\n",
    "    '''sc_l = MinMaxScaler() #StandardScaler()\n",
    "    y_trn = np.reshape(sc_l.fit_transform(np.reshape(y_trn,(-1,1))),(-1))'''\n",
    "    #y_val = np.reshape(sc_l.fit_transform(np.reshape(y_val,(-1,1))),(-1))\n",
    "    #y_tst = np.reshape(sc_l.fit_transform(np.reshape(y_tst,(-1,1))),(-1))\n",
    "\n",
    "    fullset = (x_trn, y_trn)\n",
    "    valset = (x_val, y_val)\n",
    "    testset = (x_tst, y_tst)\n",
    "\n",
    "    return fullset, valset, testset #, sc_l\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_stochastic_Facloc( data, targets, sub_budget, budget,logfile,device='cpu'):\n",
    "    \n",
    "\n",
    "    num_iterations = int(math.ceil(len(data)/sub_budget))\n",
    "    trn_indices = list(np.arange(len(data)))\n",
    "    facloc_indices = []\n",
    "\n",
    "    per_iter_bud = int(budget/num_iterations)\n",
    "\n",
    "    trn_batch = 1200\n",
    " \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        rem_indices = list(set(trn_indices).difference(set(facloc_indices)))\n",
    "\n",
    "        state = np.random.get_state()\n",
    "        np.random.seed(i*i)\n",
    "        sub_indices = np.random.choice(rem_indices, size=sub_budget, replace=False)\n",
    "        np.random.set_state(state)\n",
    "\n",
    "        data_subset = data[sub_indices].cpu()\n",
    "        targets_subset = targets[sub_indices].cpu()\n",
    "        train_loader_greedy = []\n",
    "        for item in range(math.ceil(sub_budget /trn_batch)):\n",
    "          inputs = data_subset[item*trn_batch:(item+1)*trn_batch]\n",
    "          target  = targets_subset[item*trn_batch:(item+1)*trn_batch]\n",
    "          train_loader_greedy.append((inputs,target))\n",
    "        \n",
    "        #train_loader_greedy.append((data_subset, targets_subset))\n",
    "        setf_model = SetFunctionFacLoc(device, train_loader_greedy)\n",
    "        idxs = setf_model.lazy_greedy_max(min(per_iter_bud,budget-len(facloc_indices)),logfile)#, model)\n",
    "        facloc_indices.extend([sub_indices[idx] for idx in idxs])\n",
    "    return facloc_indices\n",
    "\n",
    "\n",
    "class RegressionNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RegressionNet, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)#,bias=False)\n",
    "        self.feature_dim = input_dim\n",
    "    \n",
    "    def forward(self, x, last=False):\n",
    "        scores = self.linear(x)\n",
    "        #scores = torch.sigmoid(self.linear(x))\n",
    "        #return scores.view(-1)\n",
    "        if last:\n",
    "            return scores.view(-1), x\n",
    "        else:\n",
    "            return scores.view(-1)\n",
    "\n",
    "    def get_embedding_dim(self):\n",
    "        return self.feature_dim\n",
    "\n",
    "class LogisticNet(nn.Module):\n",
    "    def __init__(self, input_dim):#,num_cls):\n",
    "        super(LogisticNet, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim,1)\n",
    "        #self.feature_dim = input_dim\n",
    "    \n",
    "    def forward(self, x, last=False):\n",
    "        scores = self.linear(x)\n",
    "        scores = torch.sigmoid(self.linear(x).view(-1))\n",
    "        return scores\n",
    "\n",
    "\n",
    "\n",
    "def get_slices(data_name, data,labels,device,buckets=None,clean=True):\n",
    "\n",
    "    #data_slices = []\n",
    "    #abel_slices =[]\n",
    "    \n",
    "    val_data_slices = []\n",
    "    val_label_slices =[]\n",
    "\n",
    "    tst_data_slices = []\n",
    "    tst_label_slices =[]\n",
    "\n",
    "    '''sc = StandardScaler()\n",
    "    x_trn = sc.fit_transform(x_trn)\n",
    "    x_val = sc.transform(x_val)\n",
    "    x_tst = sc.transform(x_tst)'''\n",
    "\n",
    "    if data_name == 'Community_Crime_old':\n",
    "        protect_feature =[2,3,4,5]\n",
    "\n",
    "        data_class =[]\n",
    "\n",
    "        N = int(0.1*len(data)/(buckets*len(protect_feature)))\n",
    "\n",
    "        total_set = set(list(np.arange(len(data))))\n",
    "        \n",
    "        for i in protect_feature:            \n",
    "\n",
    "            digit = np.ones(data.shape[0],dtype=np.int8)*(i-1)\n",
    "            low = np.min(data[:,i])\n",
    "            high = np.max(data[:,i])\n",
    "            bins = np.linspace(low, high, buckets)\n",
    "            digitized = np.digitize(data[:,i], bins)\n",
    "            digit =  digit*10 + digitized\n",
    "\n",
    "            classes,times = np.unique(digit,return_counts=True) \n",
    "            times, classes = zip(*sorted(zip(times, classes)))\n",
    "            data_class.append(classes)\n",
    "            \n",
    "            count = 0\n",
    "            for cl in classes[:-1]:\n",
    "\n",
    "                indices=[]\n",
    "                indices_tst=[]\n",
    "                \n",
    "                idx = (digit == cl).nonzero()[0].flatten()\n",
    "                idx.tolist()\n",
    "                idxs = set(idx)\n",
    "                idxs.intersection_update(total_set)\n",
    "                idx = list(idxs)\n",
    "                #print(cl,len(idx))\n",
    "\n",
    "                curr_N = int(len(idx)/3)\n",
    "\n",
    "                #print(curr_N,N)\n",
    "                 \n",
    "                indices.extend(list(np.random.choice(idx, size=min(N,curr_N), replace=False)))\n",
    "                total_set.difference(indices)\n",
    "                idxs.difference(indices)\n",
    "                idx = list(idxs)\n",
    "                indices_tst.extend(list(np.random.choice(idx, size=min(N,curr_N), replace=False)))\n",
    "                total_set.difference(indices_tst)\n",
    "                \n",
    "                if curr_N < N:\n",
    "                    count += (N - curr_N)\n",
    "\n",
    "                val_data_slices.append(torch.from_numpy(data[indices]).float().to(device))\n",
    "                val_label_slices.append(torch.from_numpy(labels[indices]).float().to(device))\n",
    "\n",
    "                tst_data_slices.append(torch.from_numpy(data[indices_tst]).float().to(device))\n",
    "                tst_label_slices.append(torch.from_numpy(labels[indices_tst]).float().to(device))\n",
    "\n",
    "            indices=[]\n",
    "            indices_tst=[]\n",
    "            \n",
    "            idx = (digit == classes[-1]).nonzero()[0].flatten()\n",
    "            idx.tolist()\n",
    "            idxs = set(idx)\n",
    "            idxs.intersection_update(total_set)\n",
    "            idx = list(idxs)\n",
    "\n",
    "            indices.extend(list(np.random.choice(idx, size=N+count, replace=False)))\n",
    "            total_set.difference(indices)\n",
    "            idxs.difference(indices)\n",
    "            idx = list(idxs)\n",
    "            indices_tst.extend(list(np.random.choice(idx, size=N+count, replace=False)))\n",
    "            total_set.difference(indices_tst)\n",
    "\n",
    "            val_data_slices.append(torch.from_numpy(data[indices]).float().to(device))\n",
    "            val_label_slices.append(torch.from_numpy(labels[indices]).float().to(device))\n",
    "\n",
    "            tst_data_slices.append(torch.from_numpy(data[indices_tst]).float().to(device))\n",
    "            tst_label_slices.append(torch.from_numpy(labels[indices_tst]).float().to(device))\n",
    "\n",
    "        final_lables = [j for sub in data_class for j in sub]\n",
    "        left = list(total_set)    \n",
    "        data_left = data[left]\n",
    "        label_left = labels[left]\n",
    "\n",
    "    elif data_name == 'OnlineNewsPopularity':\n",
    "\n",
    "        protect_feature = [11,12,13,14,15,16]\n",
    "\n",
    "        final_lables = [ 'Lifestyle','Entertainment','Business','Social Media','Tech','World']\n",
    "\n",
    "        total_set = set(list(np.arange(len(data))))\n",
    "\n",
    "        N = int(0.1*len(data)/len(protect_feature))\n",
    "\n",
    "        max_times = 0\n",
    "        \n",
    "        for pf in protect_feature:            \n",
    "        \n",
    "            classes,times = np.unique(data[:,pf],return_counts=True) \n",
    "\n",
    "            one_id = (classes == 1.0).nonzero()[0].flatten()[0]\n",
    "\n",
    "            if max_times < times[one_id]:\n",
    "                max_times = times[one_id]\n",
    "                max_id = pf\n",
    "\n",
    "        most = final_lables[max_id-protect_feature[0]]\n",
    "        final_lables.remove(most)\n",
    "        final_lables.append(most)\n",
    "\n",
    "        count = 0\n",
    "        \n",
    "        for pf in protect_feature:\n",
    "\n",
    "            if pf == max_id:\n",
    "                continue\n",
    "            \n",
    "            idx = (data[:,pf] == 1.0).nonzero()[0].flatten()\n",
    "            idx.tolist()\n",
    "            idxs = set(idx)\n",
    "            idxs.intersection_update(total_set)\n",
    "            idx = list(idxs)\n",
    "            #print(cl,len(idx))\n",
    "\n",
    "            curr_N = int(len(idx)/3)\n",
    "\n",
    "            #print(curr_N,N)\n",
    "            \n",
    "            indices = list(np.random.choice(idx, size=min(N,curr_N), replace=False))\n",
    "            total_set.difference(indices)\n",
    "            idxs.difference(indices)\n",
    "            idx = list(idxs)\n",
    "            indices_tst = list(np.random.choice(idx, size=min(N,curr_N), replace=False))\n",
    "            total_set.difference(indices_tst)\n",
    "            \n",
    "            if curr_N < N:\n",
    "                count += (N - curr_N)\n",
    "\n",
    "            '''val_data_slices.append(torch.from_numpy(data[indices]).float().to(device))\n",
    "            val_label_slices.append(torch.from_numpy(labels[indices]).float().to(device))\n",
    "\n",
    "            tst_data_slices.append(torch.from_numpy(data[indices_tst]).float().to(device))\n",
    "            tst_label_slices.append(torch.from_numpy(labels[indices_tst]).float().to(device))'''\n",
    "\n",
    "            val_data_slices.append(data[indices])\n",
    "            val_label_slices.append(labels[indices])\n",
    "\n",
    "            tst_data_slices.append(data[indices_tst])\n",
    "            tst_label_slices.append(labels[indices_tst])\n",
    "\n",
    "            \n",
    "        idx = (data[:,max_id] == 1.0).nonzero()[0].flatten()\n",
    "        idx.tolist()\n",
    "        idxs = set(idx)\n",
    "        idxs.intersection_update(total_set)\n",
    "        idx = list(idxs)\n",
    "\n",
    "        indices = list(np.random.choice(idx, size=N+count, replace=False))\n",
    "        total_set.difference(indices)\n",
    "        idxs.difference(indices)\n",
    "        idx = list(idxs)\n",
    "        indices_tst = list(np.random.choice(idx, size=N+count, replace=False)) \n",
    "        total_set.difference(indices_tst)\n",
    "\n",
    "        '''val_data_slices.append(torch.from_numpy(data[indices]).float().to(device))\n",
    "        val_label_slices.append(torch.from_numpy(labels[indices]).float().to(device))\n",
    "\n",
    "        tst_data_slices.append(torch.from_numpy(data[indices_tst]).float().to(device))\n",
    "        tst_label_slices.append(torch.from_numpy(labels[indices_tst]).float().to(device))'''\n",
    "\n",
    "        val_data_slices.append(data[indices])\n",
    "        val_label_slices.append(labels[indices])\n",
    "\n",
    "        tst_data_slices.append(data[indices_tst])\n",
    "        tst_label_slices.append(labels[indices_tst])\n",
    "\n",
    "        left = list(total_set)\n",
    "        sc = MinMaxScaler() #StandardScaler()\n",
    "        sc_l = MinMaxScaler()\n",
    "\n",
    "        #print(data[left][0])\n",
    "        data_left = sc.fit_transform(data[left])\n",
    "        label_left = np.reshape(sc_l.fit_transform(np.reshape(labels[left],(-1,1))),(-1))\n",
    "        #print(data_left[0])\n",
    "        #preprocessing.normalize(data[left])\n",
    "\n",
    "        for j in range(len(val_data_slices)):\n",
    "            \n",
    "            val_data_slices[j] = torch.from_numpy(sc.transform(val_data_slices[j])).float().to(device)\n",
    "            tst_data_slices[j] = torch.from_numpy(sc.transform(tst_data_slices[j])).float().to(device)\n",
    "\n",
    "            val_label_slices[j] = torch.from_numpy(np.reshape(\\\n",
    "                sc_l.transform(np.reshape(val_label_slices[j],(-1,1))),(-1))).float().to(device)\n",
    "            tst_label_slices[j] = torch.from_numpy(np.reshape(\\\n",
    "                sc_l.transform(np.reshape(tst_label_slices[j],(-1,1))),(-1))).float().to(device)\n",
    "    \n",
    "    elif data_name in ['census','LawSchool','German_credit','Community_Crime']:\n",
    "        \n",
    "        if data_name == 'census':\n",
    "            protect_feature = 8 #9\n",
    "        elif data_name == 'LawSchool':\n",
    "            protect_feature = 0\n",
    "        elif data_name == 'German_credit':\n",
    "            protect_feature = 8\n",
    "        elif data_name == 'Community_Crime':\n",
    "            protect_feature = -1\n",
    "\n",
    "        total_set = set(list(np.arange(len(data))))\n",
    "        \n",
    "        classes,times = np.unique(data[:,protect_feature],return_counts=True) \n",
    "        times, classes = zip(*sorted(zip(times, classes)))\n",
    "\n",
    "        #print(times)\n",
    "        #print(classes)\n",
    "\n",
    "        N = int(0.1*len(data)/len(classes))\n",
    "        \n",
    "        count = 0\n",
    "        for cl in classes[:-1]:\n",
    "\n",
    "            indices=[]\n",
    "            indices_tst=[]\n",
    "            \n",
    "            idx = (data[:,protect_feature] == cl).nonzero()[0].flatten()\n",
    "            idx.tolist()\n",
    "\n",
    "            curr_N = int(len(idx)/3)\n",
    "\n",
    "            #print(curr_N,N)\n",
    "                \n",
    "            indices.extend(list(np.random.choice(idx, size=min(N,curr_N), replace=False)))\n",
    "            total_set.difference(indices)\n",
    "            idxs = set(idx)\n",
    "            idxs.difference(indices)\n",
    "            idx = list(idxs)\n",
    "            indices_tst.extend(list(np.random.choice(idx, size=min(N,curr_N), replace=False)))\n",
    "            total_set.difference(indices_tst)\n",
    "            \n",
    "            if curr_N < N:\n",
    "                count += (N - curr_N)\n",
    "\n",
    "            #val_data_slices.append(torch.from_numpy(preprocessing.normalize(data[indices])).float().to(device))\n",
    "            val_data_slices.append(torch.from_numpy(data[indices]).float().to(device))\n",
    "            val_label_slices.append(torch.from_numpy(labels[indices]).float().to(device))\n",
    "\n",
    "            #tst_data_slices.append(torch.from_numpy(preprocessing.normalize(data[indices_tst])).float().to(device))\n",
    "            tst_data_slices.append(torch.from_numpy(data[indices_tst]).float().to(device))\n",
    "            tst_label_slices.append(torch.from_numpy(labels[indices_tst]).float().to(device))\n",
    "\n",
    "        indices=[]\n",
    "        indices_tst=[]\n",
    "        \n",
    "        idx = (data[:,protect_feature] == classes[-1]).nonzero()[0].flatten()\n",
    "        idx.tolist()\n",
    "\n",
    "        indices.extend(list(np.random.choice(idx, size=N+count, replace=False)))\n",
    "        total_set.difference(indices)\n",
    "        idxs = set(idx)\n",
    "        idxs.difference(indices)\n",
    "        idx = list(idxs)\n",
    "        indices_tst.extend(list(np.random.choice(idx, size=N+count, replace=False)))\n",
    "        total_set.difference(indices_tst)\n",
    "\n",
    "        #val_data_slices.append(torch.from_numpy(preprocessing.normalize(data[indices])).float().to(device))\n",
    "        val_data_slices.append(torch.from_numpy(data[indices]).float().to(device))\n",
    "        val_label_slices.append(torch.from_numpy(labels[indices]).float().to(device))\n",
    "\n",
    "        #tst_data_slices.append(torch.from_numpy(preprocessing.normalize(data[indices_tst])).float().to(device))\n",
    "        tst_data_slices.append(torch.from_numpy(data[indices_tst]).float().to(device))\n",
    "        tst_label_slices.append(torch.from_numpy(labels[indices_tst]).float().to(device))\n",
    "\n",
    "        final_lables = classes\n",
    "        left = list(total_set)\n",
    "        data_left = data[left] #preprocessing.normalize(data[left]) \n",
    "        label_left = labels[left]\n",
    "\n",
    "        if not clean:\n",
    "\n",
    "            noise_size = int(len(label_left) * 0.5)\n",
    "            noise_indices = np.random.choice(np.arange(len(label_left)), size=noise_size, replace=False)\n",
    "            \n",
    "            sigma = 40\n",
    "            label_left[noise_indices] = label_left[noise_indices] + np.random.normal(0, sigma, noise_size)\n",
    "    \n",
    "        \n",
    "    return data_left, label_left, val_data_slices, val_label_slices, final_lables, tst_data_slices,\\\n",
    "        tst_label_slices,final_lables\n",
    "\n",
    "\n",
    "class CustomDataset_WithId(Dataset):\n",
    "    def __init__(self, data, target, transform=None):       \n",
    "        self.transform = transform\n",
    "        self.data = data #.astype('float32')\n",
    "        self.targets = target\n",
    "        self.X = self.data\n",
    "        self.Y = self.targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()        \n",
    "        sample_data = self.data[idx]\n",
    "        label = self.targets[idx]\n",
    "        if self.transform is not None:\n",
    "            sample_data = self.transform(sample_data)\n",
    "        return sample_data, label,idx #.astype('float32')\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                # convert float NaN --> string NaN\n",
    "                output[col] = output[col].fillna('NaN')\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)\n",
    "\n",
    "## Utility function to load datasets from libsvm datasets\n",
    "def csv_file_load(path,dim,skip=False,save_data=False):\n",
    "    data = []\n",
    "    target = []\n",
    "    with open(path) as fp:\n",
    "       if skip:\n",
    "           line = fp.readline()\n",
    "       line = fp.readline()\n",
    "       while line:\n",
    "        temp = [i for i in line.strip().split(\",\")]\n",
    "        target.append(int(float(temp[-1]))) # Class Number. # Not assumed to be in (0, K-1)\n",
    "        temp_data = [0]*dim\n",
    "        count = 0\n",
    "        for i in temp[:-1]:\n",
    "            #ind, val = i.split(':')\n",
    "            temp_data[count] = float(i)\n",
    "            count += 1\n",
    "        data.append(temp_data)\n",
    "        line = fp.readline()\n",
    "    X_data = np.array(data, dtype=np.float32)\n",
    "    Y_label = np.array(target)\n",
    "    if save_data:\n",
    "        # Save the numpy files to the folder where they come from\n",
    "        data_np_path = path + '.data.npy'\n",
    "        target_np_path = path + '.label.npy'\n",
    "        np.save(data_np_path, X_data)\n",
    "        np.save(target_np_path, Y_label)\n",
    "    return (X_data, Y_label)\n",
    "\n",
    "def libsvm_file_load(path,dim, save_data=False):\n",
    "    data = []\n",
    "    target = []\n",
    "    with open(path) as fp:\n",
    "       line = fp.readline()\n",
    "       while line:\n",
    "        temp = [i for i in line.strip().split(\" \")]\n",
    "        target.append(int(float(temp[0]))) # Class Number. # Not assumed to be in (0, K-1)\n",
    "        temp_data = [0]*dim\n",
    "        \n",
    "        for i in temp[1:]:\n",
    "            if len(i) > 1: \n",
    "                ind,val = i.split(':')\n",
    "                temp_data[int(ind)-1] = float(val)\n",
    "        data.append(temp_data)\n",
    "        line = fp.readline()\n",
    "    X_data = np.array(data,dtype=np.float32)\n",
    "    Y_label = np.array(target)\n",
    "    if save_data:\n",
    "        # Save the numpy files to the folder where they come from\n",
    "        data_np_path = path + '.data.npy'\n",
    "        target_np_path = path + '.label.npy'\n",
    "        np.save(data_np_path, X_data)\n",
    "        np.save(target_np_path, Y_label)\n",
    "    return (X_data, Y_label)\n",
    "\n",
    "def clean_lawschool_full(path):\n",
    "   \n",
    "    df = pd.read_csv(path)\n",
    "    df = df.dropna()\n",
    "    # remove y from df\n",
    "    y = df['ugpa']\n",
    "    y = y / 4\n",
    "    df = df.drop('ugpa', 1)\n",
    "    # convert gender variables to 0,1\n",
    "    df['gender'] = df['gender'].map({'male': 1, 'female': 0})\n",
    "    # add bar1 back to the feature set\n",
    "    df_bar = df['bar1']\n",
    "    df = df.drop('bar1', 1)\n",
    "    df['bar1'] = [int(grade == 'P') for grade in df_bar]\n",
    "    #df['race'] = [int(race == 7.0) for race in df['race']]\n",
    "    #a = df['race']\n",
    "    return df.to_numpy(), y.to_numpy()\n",
    "\n",
    "def majority_pop(a):\n",
    "    \"\"\"\n",
    "    Identify the main ethnicity group of each community\n",
    "    \"\"\"\n",
    "    B = \"racepctblack\"\n",
    "    W = \"racePctWhite\"\n",
    "    A = \"racePctAsian\"\n",
    "    H = \"racePctHisp\"\n",
    "    races = [B, W, A, H]\n",
    "    maj = a.apply(pd.Series.idxmax, axis=1)\n",
    "    return maj\n",
    "\n",
    "def clean_communities_full(path):\n",
    "    \"\"\"\n",
    "    Extract black and white dominant communities; \n",
    "    sub_size : number of communities for each group\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.fillna(0)\n",
    "    B = \"racepctblack\"\n",
    "    W = \"racePctWhite\"\n",
    "    A = \"racePctAsian\"\n",
    "    H = \"racePctHisp\"\n",
    "    sens_features = [2, 3, 4, 5]\n",
    "    df_sens = df.iloc[:, sens_features]\n",
    "\n",
    "    # creating labels using crime rate\n",
    "    Y = df['ViolentCrimesPerPop']\n",
    "    df = df.drop('ViolentCrimesPerPop', 1)\n",
    "\n",
    "    maj = majority_pop(df_sens)\n",
    "\n",
    "    # remap the values of maj\n",
    "    a = maj.map({B : 0, W : 1, A : 2, H : 3})\n",
    "   \n",
    "    df['race'] = a\n",
    "    df = df.drop(H, 1)\n",
    "    df = df.drop(B, 1)\n",
    "    df = df.drop(W, 1)\n",
    "    df = df.drop(A, 1)\n",
    "\n",
    "    #print(df.head())\n",
    "\n",
    "    return df.to_numpy(), Y.to_numpy()\n",
    "\n",
    "def house_price_load(trn_path,tst_path,save_data=False):\n",
    "\n",
    "    train_csv = pd.read_csv(trn_path)\n",
    "    test_csv = pd.read_csv(tst_path)\n",
    "\n",
    "    drop_columns = (train_csv.isnull().sum().sort_values(ascending=False).\\\n",
    "        loc[lambda x : x> .90*1460]).index.to_list()\n",
    "\n",
    "    train_clean = train_csv.drop(drop_columns, axis = 'columns', errors = 'ignore')\n",
    "    test_clean = test_csv.drop(drop_columns, axis = 'columns', errors = 'ignore')\n",
    "\n",
    "    train_10_percent_missing_features = train_clean.isnull().sum().sort_values(ascending=False).loc[lambda x : (x<.10*1460)  & (x != 0)].index.to_list()\n",
    "    train_10_percent_missing_features_cat = train_clean[train_10_percent_missing_features].select_dtypes('object').columns.to_list()\n",
    "    train_10_percent_missing_features_num = train_clean[train_10_percent_missing_features].select_dtypes('number').columns.to_list()\n",
    "\n",
    "    train_clean[train_10_percent_missing_features_cat] = train_clean[train_10_percent_missing_features_cat].fillna(train_clean[train_10_percent_missing_features_cat].mode().iloc[0])\n",
    "    train_clean[train_10_percent_missing_features_num] = train_clean[train_10_percent_missing_features_num].fillna(train_clean[train_10_percent_missing_features_num].median().iloc[0])\n",
    "\n",
    "    test_10_percent_missing_features = test_clean.isnull().sum().sort_values(ascending=False).loc[lambda x : (x<.10*1460)  & (x != 0)].index.to_list()\n",
    "    test_10_percent_missing_features_cat = test_clean[test_10_percent_missing_features].select_dtypes('object').columns.to_list()\n",
    "    test_10_percent_missing_features_num = test_clean[test_10_percent_missing_features].select_dtypes('number').columns.to_list()\n",
    "\n",
    "    test_clean[test_10_percent_missing_features_cat] = test_clean[test_10_percent_missing_features_cat].fillna(test_clean[test_10_percent_missing_features_cat].mode().iloc[0])\n",
    "    test_clean[test_10_percent_missing_features_num] = test_clean[test_10_percent_missing_features_num].fillna(test_clean[test_10_percent_missing_features_num].median().iloc[0])\n",
    "\n",
    "    train_clean[\"LotFrontage\"] = train_clean[\"LotFrontage\"].fillna(train_clean[\"LotFrontage\"].median())\n",
    "    test_clean[\"LotFrontage\"] = test_clean[\"LotFrontage\"].fillna(test_clean[\"LotFrontage\"].median())\n",
    "\n",
    "    train_clean.drop('Id', axis = 1, inplace = True)\n",
    "    test_clean.drop('Id', axis = 1, inplace = True)\n",
    "\n",
    "    trn_y = train_clean['SalePrice'].to_frame()\n",
    "    #tst_y = test_clean['SalePrice'].to_frame()\n",
    "\n",
    "    train_clean.drop(['SalePrice'], axis = 1, inplace = True)\n",
    "    #test_clean.drop(['SalePrice'], axis = 1, inplace = True)\n",
    " \n",
    "    X_train_clean_index = train_clean.index.to_list()\n",
    "    X_total = train_clean.append(test_clean,ignore_index = True)\n",
    "    X_test_clean_index = np.setdiff1d(X_total.index.to_list() ,X_train_clean_index)\n",
    "\n",
    "    cat_features = X_total.select_dtypes(['object']).columns.to_list()\n",
    "    X_total_encoded = MultiColumnLabelEncoder(columns = cat_features).fit_transform(X_total)\n",
    "    \n",
    "    X_train_clean_encoded = X_total_encoded.iloc[X_train_clean_index, :]\n",
    "    X_test_clean_encoded = X_total_encoded.iloc[X_test_clean_index, :].reset_index(drop = True) \n",
    "    \n",
    "    return X_train_clean_encoded.to_numpy(),trn_y.to_numpy()#,X_test_clean_encoded,tst_y\n",
    "\n",
    "def community_crime_load(path,dim, save_data=False):\n",
    "\n",
    "    data = []\n",
    "    target = []\n",
    "    with open(path) as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            temp = [i.strip() for i in line.strip().split(\",\")][5:]\n",
    "\n",
    "            target.append(float(temp[-1]))\n",
    "            \n",
    "            temp_data = [0.0]*dim\n",
    "            \n",
    "            #print(temp)\n",
    "\n",
    "            for i in range(len(temp[:-1])):\n",
    "\n",
    "                if temp[i] != '?':\n",
    "                    temp_data[i] = float(temp[i])\n",
    "            \n",
    "            data.append(temp_data)\n",
    "            line = fp.readline()\n",
    "    \n",
    "    X_data = np.array(data, dtype=np.float32)\n",
    "    Y_label = np.array(target)\n",
    "    \n",
    "    if save_data:\n",
    "        # Save the numpy files to the folder where they come from\n",
    "        data_np_path = path + '.data.npy'\n",
    "        target_np_path = path + '.label.npy'\n",
    "        np.save(data_np_path, X_data)\n",
    "        np.save(target_np_path, Y_label)\n",
    "    return (X_data, Y_label)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, target, device=None, transform=None):       \n",
    "        self.transform = transform\n",
    "        if device is not None:\n",
    "            # Push the entire data to given device, eg: cuda:0\n",
    "            self.data = torch.from_numpy(data.astype('float32')).to(device)\n",
    "            self.targets = torch.from_numpy(target).to(device)\n",
    "        else:\n",
    "            self.data = data#.astype('float32')\n",
    "            self.targets = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()        \n",
    "        sample_data = self.data[idx]\n",
    "        label = self.targets[idx]\n",
    "        if self.transform is not None:\n",
    "            sample_data = self.transform(sample_data)\n",
    "        return (sample_data, label) #.astype('float32')\n",
    "\n",
    "def load_dataset_custom (datadir, dset_name,isnumpy=True):\n",
    "\n",
    "    if dset_name == \"Community_Crime\":\n",
    "\n",
    "        x_trn, y_trn = clean_communities_full(os.path.join(datadir, 'communities.csv'))\n",
    "\n",
    "        if isnumpy:\n",
    "            fullset = (x_trn, y_trn)\n",
    "            \n",
    "        else:\n",
    "            fullset = CustomDataset(x_trn, y_trn)\n",
    "\n",
    "        return fullset, x_trn.shape[1] \n",
    "\n",
    "    elif dset_name == 'LawSchool':\n",
    "\n",
    "        x_trn, y_trn = clean_lawschool_full(os.path.join(datadir, 'lawschool.csv'))\n",
    "\n",
    "        if isnumpy:\n",
    "            fullset = (x_trn, y_trn)\n",
    "\n",
    "        else:\n",
    "            fullset = CustomDataset(x_trn, y_trn)\n",
    "            \n",
    "        return fullset, x_trn.shape[1] \n",
    "\n",
    "def load_std_regress_data (datadir, dset_name,isnumpy=True,clean=True):\n",
    "\n",
    "    if dset_name == \"cadata\":\n",
    "        trn_file = os.path.join(datadir, 'cadata.txt')\n",
    "        x_trn, y_trn  = libsvm_file_load(trn_file,8)\n",
    "    \n",
    "    elif dset_name == \"abalone\":\n",
    "        trn_file = os.path.join(datadir, 'abalone_scale.txt')\n",
    "        x_trn, y_trn  = libsvm_file_load(trn_file,8)\n",
    "\n",
    "    elif dset_name == \"cpusmall\":\n",
    "        trn_file = os.path.join(datadir, 'cpusmall_scale.txt')\n",
    "        x_trn, y_trn  = libsvm_file_load(trn_file,12)\n",
    "\n",
    "    elif dset_name == \"housing\":\n",
    "        trn_file = os.path.join(datadir, 'housing_scale.txt')\n",
    "        x_trn, y_trn  = libsvm_file_load(trn_file,13)\n",
    "\n",
    "    elif dset_name == \"mg\":\n",
    "        trn_file = os.path.join(datadir, 'mg_scale.txt')\n",
    "        x_trn, y_trn  = libsvm_file_load(trn_file,6)\n",
    "\n",
    "    elif dset_name == \"MSD\":\n",
    "        trn_file = os.path.join(datadir, 'YearPredictionMSD')\n",
    "        x_trn, y_trn  = libsvm_file_load(trn_file,90)\n",
    "\n",
    "    elif dset_name == \"house_pricing\":\n",
    "        trn_file = os.path.join(datadir, 'train.csv')\n",
    "        test_file = os.path.join(datadir, 'test.csv')\n",
    "        x_trn, y_trn  = house_price_load(trn_file,test_file) #,x_tst,y_tst\n",
    "\n",
    "    elif dset_name == \"synthetic\":\n",
    "\n",
    "        data_dims = 30 #100\n",
    "        samples = 5000#1000000\n",
    "\n",
    "        np.random.seed(42)\n",
    "\n",
    "        avg = np.zeros(data_dims)\n",
    "        cov = np.identity(data_dims)\n",
    "\n",
    "        x_trn = np.random.multivariate_normal(avg, cov, samples)#.T\n",
    "\n",
    "        w = np.random.normal(0,10, data_dims) #0.25\n",
    "\n",
    "        sigma = 30\n",
    "\n",
    "        y_trn = x_trn.dot(w) + np.random.normal(0, sigma, samples)\n",
    "    \n",
    "    #elif dset_name == \"house_pricing\":\n",
    "    #    x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.1, random_state=42)\n",
    "\n",
    "    \n",
    "    if dset_name == \"MSD\":\n",
    "        tst_file = os.path.join(datadir, 'YearPredictionMSD.t')\n",
    "        x_tst, y_tst  = libsvm_file_load(tst_file,90)\n",
    "        x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.005, random_state=42)\n",
    "    else:\n",
    "        x_trn, x_tst, y_trn, y_tst = train_test_split(x_trn, y_trn, test_size=0.1, random_state=42)\n",
    "        x_trn, x_val, y_trn, y_val = train_test_split(x_trn, y_trn, test_size=0.1, random_state=42)\n",
    "\n",
    "    if not clean:\n",
    "\n",
    "        noise_size = int(len(y_trn) * 0.5)\n",
    "        noise_indices = np.random.choice(np.arange(len(y_trn)), size=noise_size, replace=False)\n",
    "        \n",
    "        sigma = 40\n",
    "        y_trn[noise_indices] = y_trn[noise_indices] + np.random.normal(0, sigma, noise_size)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    x_trn = sc.fit_transform(x_trn)\n",
    "    x_val = sc.transform(x_val)\n",
    "    x_tst = sc.transform(x_tst)\n",
    "\n",
    "    sc_l = StandardScaler()\n",
    "    y_trn = np.reshape(sc_l.fit_transform(np.reshape(y_trn,(-1,1))),(-1))\n",
    "    y_val = np.reshape(sc_l.fit_transform(np.reshape(y_val,(-1,1))),(-1))\n",
    "    y_tst = np.reshape(sc_l.fit_transform(np.reshape(y_tst,(-1,1))),(-1))\n",
    "\n",
    "    if isnumpy:\n",
    "        fullset = (x_trn, y_trn)\n",
    "        valset = (x_val, y_val)\n",
    "        testset = (x_tst, y_tst)\n",
    "\n",
    "    else:\n",
    "        fullset = CustomDataset(x_trn, y_trn)\n",
    "        valset = CustomDataset(x_val, y_val)\n",
    "        testset = CustomDataset(x_tst, y_tst)\n",
    "\n",
    "    return fullset, valset, testset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Regression():\n",
    "    def __init__(self):\n",
    "        self.select_every = 35\n",
    "        self.reg_lambda = 1e-5\n",
    "        self.val_loss = 0\n",
    "        self.test_loss = 0.\n",
    "        self.test_loss_std = 0\n",
    "        self.batch_size = 4000\n",
    "        self.learning_rate = 0.01\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.psuedo_length = 1.0\n",
    "        self.subset_idx = None\n",
    "        \n",
    "    def weight_reset(self, m):\n",
    "        '''\n",
    "        Fills the input tensor using Glorot Initialisation and\n",
    "\n",
    "        '''\n",
    "        torch.manual_seed(42)\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.1)\n",
    "        \n",
    "    def train_model(self, x_trn, y_trn, x_val, y_val, fraction, delt = [0.3], x_tst = None, y_tst = None,\\\n",
    "                    num_epochs=2000, default = False, ebud=None):\n",
    "        \n",
    "        sub_epoch = 3\n",
    "        \n",
    "        N, M = x_trn.shape\n",
    "        bud = int(fraction * N)\n",
    "        print(\"Budget, fraction and N:\", bud, fraction, N)\n",
    "        train_batch_size = min(bud,1000)\n",
    "        print_every = 50\n",
    "\n",
    "        deltas = torch.tensor(delt).to(self.device) \n",
    "        \n",
    "      # initialise index\n",
    "        rand_idxs = list(np.random.choice(N, size=bud, replace=False))\n",
    "        idxs = rand_idxs\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Initialise model\n",
    "        main_model = RegressionNet(M)\n",
    "        main_model.apply(self.weight_reset)\n",
    "\n",
    "        #for p in main_model.parameters():\n",
    "        #    print(p.data)\n",
    "\n",
    "        main_model = main_model.to(self.device)\n",
    "        #criterion_sum = nn.MSELoss(reduction='sum')\n",
    "        #main_optimizer = optim.SGD(main_model.parameters(), lr=learning_rate)\n",
    "        main_optimizer = torch.optim.Adam(main_model.parameters(), lr = self.learning_rate)\n",
    "\n",
    "        #scheduler = torch.optim.lr_scheduler.MultiStepLR(main_optimizer, milestones=change, gamma=0.5)\n",
    "        scheduler = optim.lr_scheduler.StepLR(main_optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "        #print(idxs)\n",
    "\n",
    "        idxs.sort()\n",
    "        np.random.seed(42)\n",
    "        np_sub_idxs = np.array(idxs)\n",
    "        np.random.shuffle(np_sub_idxs)\n",
    "        loader_tr = DataLoader(CustomDataset(x_trn[np_sub_idxs], y_trn[np_sub_idxs],\\\n",
    "                transform=None),shuffle=False,batch_size=train_batch_size)\n",
    "\n",
    "        loader_full_tr = DataLoader(CustomDataset(x_trn, y_trn,transform=None),shuffle=False,\\\n",
    "            batch_size=train_batch_size)\n",
    "\n",
    "        loader_val = DataLoader(CustomDataset(x_val, y_val,transform=None),shuffle=False,\\\n",
    "            batch_size=self.batch_size)\n",
    "        \n",
    "        cached_state_dict = copy.deepcopy(main_model.state_dict())\n",
    "\n",
    "        if self.psuedo_length == 1.0:\n",
    "            sub_rand_idxs = [s for s in range(N)]\n",
    "            current_idxs = idxs\n",
    "        else:\n",
    "            sub_rand_idxs = [s for s in range(N)]\n",
    "            new_ele = set(sub_rand_idxs).difference(set(idxs))\n",
    "            sub_rand_idxs = list(np.random.choice(list(new_ele), size=int(self.psuedo_length*N), replace=False))\n",
    "            \n",
    "            sub_rand_idxs = idxs + sub_rand_idxs\n",
    "\n",
    "            current_idxs = [s for s in range(len(idxs))]\n",
    "\n",
    "        fsubset_d = FindSubset_Vect_TrnLoss(x_trn[sub_rand_idxs], y_trn[sub_rand_idxs], x_val, y_val,main_model,\\\n",
    "            criterion,self.device,deltas,self.learning_rate,self.reg_lambda,self.batch_size)\n",
    "\n",
    "        fsubset_d.precompute(int(num_epochs/4),sub_epoch,torch.randn_like(deltas,device=self.device))\n",
    "\n",
    "        main_model.load_state_dict(cached_state_dict)\n",
    "        \n",
    "        print(\"Starting Subset of size \",fraction,\" with fairness Run!\")\n",
    "        \n",
    "        stop_count = 0\n",
    "        prev_loss = 1000\n",
    "        prev_loss2 = 1000\n",
    "        i =0\n",
    "        mul=1\n",
    "        lr_count = 0\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "        \n",
    "            temp_loss = 0.\n",
    "\n",
    "            for batch_idx in list(loader_tr.batch_sampler):\n",
    "\n",
    "                inputs, targets = loader_tr.dataset[batch_idx]\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                main_optimizer.zero_grad()\n",
    "\n",
    "                scores = main_model(inputs)\n",
    "\n",
    "                l2_reg = 0\n",
    "                for param in main_model.parameters():\n",
    "                    l2_reg += torch.norm(param)\n",
    "\n",
    "                '''l = [torch.flatten(p) for p in main_model.parameters()]\n",
    "                flat = torch.cat(l)\n",
    "                l2_reg = torch.sum(flat*flat)'''\n",
    "\n",
    "                loss = criterion(scores, targets) +  self.reg_lambda*l2_reg*len(batch_idx)\n",
    "                temp_loss += loss.item()\n",
    "                loss.backward()\n",
    "\n",
    "                for p in filter(lambda p: p.grad is not None, main_model.parameters()):\\\n",
    "                     p.grad.data.clamp_(min=-.1, max=.1)\n",
    "\n",
    "                main_optimizer.step()\n",
    "                #scheduler.step()\n",
    "\n",
    "            if i % print_every == 0:  # Print Training and Validation Loss\n",
    "                print('Epoch:', i + 1, 'SubsetTrn', temp_loss)\n",
    "                print(\"Previous loss: \", prev_loss, \"\\n\"\\\n",
    "                    ,\"Temporary loss: \", temp_loss, \"\\n\"\\\n",
    "                    , \"Mul: \", mul)\n",
    "                # print(main_optimizer.param_groups[0]['lr'])\n",
    "                \n",
    "            if ((i + 1) % self.select_every == 0):\n",
    "                \n",
    "                cached_state_dict = copy.deepcopy(main_model.state_dict())\n",
    "                clone_dict = copy.deepcopy(cached_state_dict)\n",
    "                \n",
    "                fsubset_d.lr = main_optimizer.param_groups[0]['lr']*mul#,1e-4)\n",
    "\n",
    "                state_values = list(main_optimizer.state.values())\n",
    "                step = state_values[0]['step']\n",
    "\n",
    "                w_exp_avg = torch.cat((state_values[0]['exp_avg'].view(-1),state_values[1]['exp_avg']))\n",
    "                #torch.zeros(x_trn.shape[1]+1,device=self.device)\n",
    "                w_exp_avg_sq = torch.cat((state_values[0]['exp_avg_sq'].view(-1),state_values[1]['exp_avg_sq']))\n",
    "                #torch.zeros(x_trn.shape[1]+1,device=self.device)\n",
    "\n",
    "                #a_exp_avg = torch.zeros(1,device=self.device)\n",
    "                #a_exp_avg_sq = torch.zeros(1,device=self.device)\n",
    "\n",
    "                #print(exp_avg,exp_avg_sq)\n",
    "\n",
    "                d_sub_idxs = fsubset_d.return_subset(clone_dict,sub_epoch,current_idxs,\\\n",
    "                    bud,train_batch_size,step,w_exp_avg,w_exp_avg_sq)\n",
    "                #torch.ones_like(deltas,device=self.device),a_exp_avg,a_exp_avg_sq)#,main_optimizer,dual_optimizer)\n",
    "\n",
    "                '''clone_dict = copy.deepcopy(cached_state_dict)\n",
    "                alpha_orig = copy.deepcopy(alphas)\n",
    "\n",
    "                sub_idxs = fsubset.return_subset(clone_dict,sub_epoch,sub_idxs,alpha_orig,bud,\\\n",
    "                    train_batch_size)\n",
    "                print(sub_idxs[:10])'''\n",
    "\n",
    "                current_idxs = d_sub_idxs\n",
    "                #print(len(d_sub_idxs))\n",
    "\n",
    "                d_sub_idxs = list(np.array(sub_rand_idxs)[d_sub_idxs])\n",
    "                \n",
    "                new_ele = set(d_sub_idxs).difference(set(idxs))\n",
    "                # print(len(new_ele),0.1*bud)\n",
    "\n",
    "                if len(new_ele) > 0.1*bud:\n",
    "                    main_optimizer = torch.optim.Adam([\n",
    "                    {'params': main_model.parameters()}], lr=main_optimizer.param_groups[0]['lr']*mul)\n",
    "                    #max(main_optimizer.param_groups[0]['lr'],0.001))\n",
    "\n",
    "                    #mul=1\n",
    "                    #stop_count = 0\n",
    "                    lr_count = 0\n",
    "\n",
    "                idxs = d_sub_idxs\n",
    "\n",
    "                idxs.sort()\n",
    "\n",
    "                #print(idxs[:10])\n",
    "                np.random.seed(42)\n",
    "                np_sub_idxs = np.array(idxs)\n",
    "                np.random.shuffle(np_sub_idxs)\n",
    "                loader_tr = DataLoader(CustomDataset(x_trn[np_sub_idxs], y_trn[np_sub_idxs],\\\n",
    "                        transform=None),shuffle=False,batch_size=train_batch_size)\n",
    "\n",
    "                main_model.load_state_dict(cached_state_dict)\n",
    "\n",
    "            if abs(prev_loss - temp_loss) <= 1e-1*mul or prev_loss2 == temp_loss:\n",
    "                #print(main_optimizer.param_groups[0]['lr'])\n",
    "                lr_count += 1\n",
    "                if lr_count == 10:\n",
    "                    #print(i,\"Reduced\")\n",
    "                    # print(prev_loss,temp_loss,main_optimizer.param_groups[0]['lr'])\n",
    "                    scheduler.step()\n",
    "                    mul/=10\n",
    "                    lr_count = 0\n",
    "            else:\n",
    "                lr_count = 0\n",
    "\n",
    "            prev_loss2 = prev_loss\n",
    "            prev_loss = temp_loss\n",
    "        \n",
    "        no_red_error = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "        main_model.eval()\n",
    "\n",
    "        l = [torch.flatten(p) for p in main_model.parameters()]\n",
    "        flat = torch.cat(l)\n",
    "\n",
    "        print('Subset_fair',len(idxs))\n",
    "        # print(flat)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            #self.val_loss = 0.\n",
    "            for batch_idx in list(loader_val.batch_sampler):\n",
    "\n",
    "                inputs, targets = loader_val.dataset[batch_idx]\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                val_out = main_model(inputs)\n",
    "                '''if is_time:\n",
    "                    val_out = sc_trans.inverse_transform(val_out.cpu().numpy())\n",
    "                    val_out = torch.from_numpy(val_out).float()'''\n",
    "\n",
    "                if batch_idx[0] == 0:\n",
    "                    e_val_loss = no_red_error(val_out, targets)\n",
    "\n",
    "                else:\n",
    "                    batch_val_loss = no_red_error(val_out, targets)\n",
    "                    e_val_loss = torch.cat((e_val_loss, batch_val_loss),dim= 0)\n",
    "\n",
    "            #val_loss /= len(loader_val.batch_sampler)\n",
    "            self.val_loss = torch.mean(e_val_loss)\n",
    "            # print(list(e_val_loss.cpu().numpy()))\n",
    "            \n",
    "            if (default == True):\n",
    "                \n",
    "                loader_tst = DataLoader(CustomDataset(x_tst, y_tst,transform=None),shuffle=False,\\\n",
    "                                        batch_size=self.batch_size)\n",
    "\n",
    "                for batch_idx in list(loader_tst.batch_sampler):\n",
    "\n",
    "                    inputs, targets = loader_tst.dataset[batch_idx]\n",
    "                    inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                    outputs = main_model(inputs)\n",
    "                    '''if is_time:\n",
    "                        outputs = sc_trans.inverse_transform(outputs.cpu().numpy())\n",
    "                        outputs = torch.from_numpy(outputs).float()'''\n",
    "                    #test_loss += criterion(outputs, targets)\n",
    "\n",
    "                    if batch_idx[0] == 0:\n",
    "                        e_tst_loss = no_red_error(outputs, targets)\n",
    "\n",
    "                    else:\n",
    "                        batch_tst_loss = no_red_error(outputs, targets)\n",
    "                        e_tst_loss = torch.cat((e_tst_loss, batch_tst_loss),dim= 0)\n",
    "\n",
    "                #test_loss /= len(loader_tst.batch_sampler)    \n",
    "                self.test_loss = torch.mean(e_tst_loss)\n",
    "                self.test_loss_std = torch.std(e_tst_loss)\n",
    "\n",
    "                # print(list(e_tst_loss.cpu().numpy()))\n",
    "                \n",
    "    def train_model_fair(self, x_trn, y_trn, x_val, y_val, fraction,  delt = [0.3], x_tst = None, y_tst = None,\\\n",
    "                    num_epochs=2000, default = False, bud=None):\n",
    "\n",
    "        sub_epoch = 3\n",
    "        \n",
    "        N, M = x_trn.shape\n",
    "        bud = int(fraction * N)\n",
    "        print(\"Budget, fraction and N:\", bud, fraction, N)\n",
    "        train_batch_size = min(bud,1000)\n",
    "        print_every = 50\n",
    "\n",
    "        deltas = torch.tensor(delt).to(self.device) \n",
    "        \n",
    "      # initialise index\n",
    "        rand_idxs = list(np.random.choice(N, size=bud, replace=False))\n",
    "        sub_idxs = rand_idxs\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        main_model = RegressionNet(M)\n",
    "        main_model.apply(self.weight_reset)\n",
    "\n",
    "        #for p in main_model.parameters():\n",
    "        #    print(p.data)\n",
    "\n",
    "        main_model = main_model.to(self.device)\n",
    "\n",
    "        #criterion_sum = nn.MSELoss(reduction='sum')\n",
    "\n",
    "        alphas = torch.randn_like(deltas,device=self.device) #+ 5. #,requires_grad=True)\n",
    "        alphas.requires_grad = True\n",
    "        #print(alphas)\n",
    "        #alphas = torch.ones_like(deltas,requires_grad=True)\n",
    "        '''main_optimizer = optim.SGD([{'params': main_model.parameters()},\n",
    "                    {'params': alphas}], lr=learning_rate) #'''\n",
    "        main_optimizer = torch.optim.Adam(main_model.parameters(), lr=self.learning_rate)\n",
    "        #[{'params': main_model.parameters()}], lr=learning_rate)\n",
    "\n",
    "        dual_optimizer = torch.optim.Adam([{'params': alphas}], lr=self.learning_rate) #{'params': alphas} #'''\n",
    "\n",
    "        #scheduler = torch.optim.lr_scheduler.MultiStepLR(main_optimizer, milestones=change,\\\n",
    "        #     gamma=0.5) #[e*2 for e in change]\n",
    "        scheduler = optim.lr_scheduler.StepLR(main_optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "        #alphas.requires_grad = False\n",
    "\n",
    "        #delta_extend = torch.repeat_interleave(deltas,val_size, dim=0)    \n",
    "\n",
    "        \n",
    "\n",
    "        cached_state_dict = copy.deepcopy(main_model.state_dict())\n",
    "        alpha_orig = copy.deepcopy(alphas)\n",
    "\n",
    "        if self.psuedo_length == 1.0:\n",
    "            sub_rand_idxs = [s for s in range(N)]\n",
    "            current_idxs = sub_idxs\n",
    "        else:\n",
    "            sub_rand_idxs = [s for s in range(N)]\n",
    "            new_ele = set(sub_rand_idxs).difference(set(sub_idxs))\n",
    "            sub_rand_idxs = list(np.random.choice(list(new_ele), size=int(self.psuedo_length*N), replace=False))\n",
    "\n",
    "            sub_rand_idxs = sub_idxs + sub_rand_idxs\n",
    "\n",
    "            current_idxs = [s for s in range(len(sub_idxs))]\n",
    "\n",
    "        fsubset_d = FindSubset_Vect(x_trn[sub_rand_idxs], y_trn[sub_rand_idxs], x_val, y_val,main_model,\\\n",
    "            criterion,self.device,deltas,self.learning_rate,self.reg_lambda,self.batch_size)\n",
    "\n",
    "        fsubset_d.precompute(int(num_epochs/4),sub_epoch,alpha_orig)\n",
    "\n",
    "        '''main_model.load_state_dict(cached_state_dict)\n",
    "        alpha_orig = copy.deepcopy(alphas)\n",
    "\n",
    "        fsubset = FindSubset(x_trn, y_trn, x_val, y_val,main_model,criterion,\\\n",
    "            self.device,deltas,learning_rate,reg_lambda)\n",
    "\n",
    "        fsubset.precompute(int(num_epochs/4),sub_epoch,alpha_orig,batch_size)'''\n",
    "\n",
    "        main_model.load_state_dict(cached_state_dict)\n",
    "\n",
    "        print(\"Starting Subset of size \",fraction,\" with fairness Run!\")\n",
    "\n",
    "        sub_idxs.sort()\n",
    "        np.random.seed(42)\n",
    "        np_sub_idxs = np.array(sub_idxs)\n",
    "        np.random.shuffle(np_sub_idxs)\n",
    "        loader_tr = DataLoader(CustomDataset(x_trn[np_sub_idxs], y_trn[np_sub_idxs],\\\n",
    "                transform=None),shuffle=False,batch_size=train_batch_size)\n",
    "\n",
    "        loader_val = DataLoader(CustomDataset(x_val, y_val,transform=None),shuffle=False,\\\n",
    "            batch_size=self.batch_size)\n",
    "\n",
    "        loader_tst = DataLoader(CustomDataset(x_tst, y_tst,transform=None),shuffle=False,\\\n",
    "            batch_size=self.batch_size)\n",
    "\n",
    "        stop_epoch = num_epochs\n",
    "\n",
    "        #for i in range(num_epochs):\n",
    "        stop_count = 0\n",
    "        prev_loss = 1000\n",
    "        prev_loss2 = 1000\n",
    "        i =0\n",
    "        mul = 1\n",
    "        lr_count = 0\n",
    "        #while (True):\n",
    "        for i in range(num_epochs):\n",
    "\n",
    "            # inputs, targets = x_trn[idxs].to(self.device), y_trn[idxs].to(self.device)\n",
    "            #inputs, targets = x_trn[sub_idxs], y_trn[sub_idxs]\n",
    "\n",
    "            temp_loss = 0.\n",
    "\n",
    "            starting = time.process_time() \n",
    "\n",
    "            for batch_idx_t in list(loader_tr.batch_sampler):\n",
    "\n",
    "                inputs_trn, targets_trn = loader_tr.dataset[batch_idx_t]\n",
    "                inputs_trn, targets_trn = inputs_trn.to(self.device), targets_trn.to(self.device)\n",
    "\n",
    "                main_optimizer.zero_grad()\n",
    "\n",
    "                scores_trn = main_model(inputs_trn)\n",
    "\n",
    "                l2_reg = 0\n",
    "                for param in main_model.parameters():\n",
    "                    l2_reg += torch.norm(param)\n",
    "\n",
    "                '''l = [torch.flatten(p) for p in main_model.parameters()]\n",
    "                flat = torch.cat(l)\n",
    "                l2_reg = torch.sum(flat*flat)'''\n",
    "\n",
    "                #state_orig = copy.deepcopy(main_optimizer.state)\n",
    "\n",
    "                '''alpha_extend = torch.repeat_interleave(alphas,val_size, dim=0)\n",
    "                val_scores = main_model(x_val_combined)\n",
    "                constraint = criterion(val_scores, y_val_combined) - delta_extend\n",
    "                multiplier = torch.dot(alpha_extend,constraint)'''\n",
    "\n",
    "                '''constraint = torch.zeros(len(x_val_list))\n",
    "                for j in range(len(x_val_list)):\n",
    "\n",
    "                    inputs_j, targets_j = x_val_list[j], y_val_list[j]\n",
    "                    scores_j = main_model(inputs_j)\n",
    "                    constraint[j] = criterion(scores_j, targets_j) - deltas[j]'''\n",
    "\n",
    "                constraint = 0.\n",
    "                for batch_idx in list(loader_val.batch_sampler):\n",
    "\n",
    "                    inputs, targets = loader_val.dataset[batch_idx]\n",
    "                    inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                    val_out = main_model(inputs)\n",
    "                    '''if is_time:\n",
    "                        val_out = sc_trans.inverse_transform(val_out.cpu().numpy())\n",
    "                        val_out = torch.from_numpy(val_out).float()'''\n",
    "                    constraint += criterion(val_out, targets)            \n",
    "\n",
    "                constraint /= len(loader_val.batch_sampler)\n",
    "                constraint = constraint - deltas\n",
    "                multiplier = alphas*constraint*(float(constraint > 0)) #torch.dot(alphas,constraint)\n",
    "\n",
    "                loss = criterion(scores_trn, targets_trn) + self.reg_lambda*l2_reg*len(batch_idx_t) + \\\n",
    "                    multiplier #\n",
    "                temp_loss += loss.item()\n",
    "                loss.backward()\n",
    "\n",
    "                #if i % print_every == 0:  \n",
    "                #    print(criterion(scores_trn, targets_trn) , reg_lambda*l2_reg*len(batch_idx_t) ,multiplier)\n",
    "\n",
    "                # clamp gradients, just in case\n",
    "                for p in filter(lambda p: p.grad is not None, main_model.parameters()):\\\n",
    "                     p.grad.data.clamp_(min=-.1, max=.1)\n",
    "\n",
    "                main_optimizer.step()\n",
    "                #scheduler.step()\n",
    "                #main_optimizer.param_groups[1]['lr'] = learning_rate/2\n",
    "\n",
    "                '''for param in main_model.parameters():\n",
    "                    param.requires_grad = False\n",
    "                alphas.requires_grad = True'''\n",
    "\n",
    "                dual_optimizer.zero_grad()\n",
    "\n",
    "                #if constraint > 0:\n",
    "                constraint = 0.\n",
    "                for batch_idx in list(loader_val.batch_sampler):\n",
    "\n",
    "                    inputs, targets = loader_val.dataset[batch_idx]\n",
    "                    inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                    val_out = main_model(inputs)\n",
    "                    '''if is_time:\n",
    "                        val_out = sc_trans.inverse_transform(val_out.cpu().numpy())\n",
    "                        val_out = torch.from_numpy(val_out).float()'''\n",
    "                    constraint += criterion(val_out, targets)\n",
    "\n",
    "                constraint /= len(loader_val.batch_sampler)\n",
    "                constraint = constraint - deltas\n",
    "                multiplier = -1.0*alphas*constraint*(float(constraint > 0)) #torch.dot(-1.0*alphas ,constraint)\n",
    "\n",
    "                #print(alphas,constraint)\n",
    "\n",
    "                #main_optimizer.state = state_orig\n",
    "                multiplier.backward()\n",
    "                dual_optimizer.step()\n",
    "                #print(main_optimizer.param_groups)\n",
    "                #scheduler.step()`\n",
    "\n",
    "                alphas.requires_grad = False\n",
    "                alphas.clamp_(min=0.0)\n",
    "                alphas.requires_grad = True\n",
    "                #print(alphas)\n",
    "\n",
    "                '''for param in main_model.parameters():\n",
    "                    param.requires_grad = True'''\n",
    "\n",
    "            #print(alphas,constraint)\n",
    "\n",
    "            if i % print_every == 0:  # Print Training and Validation Loss\n",
    "                print('Epoch:', i + 1, 'SubsetTrn', loss.item())\n",
    "                print(\"Previous loss: \", prev_loss, \"\\n\"\\\n",
    "                    ,\"Temporary loss: \", temp_loss, \"\\n\"\\\n",
    "                    ,\"Constraint: \", constraint, \"\\n\"\\\n",
    "                    , \"Alphas: \", alphas)\n",
    "                #print(main_optimizer.state)#.keys())\n",
    "                #print(alphas,constraint)\n",
    "                #print(criterion(scores, targets) , reg_lambda*l2_reg*len(idxs) ,multiplier)\n",
    "                #print(main_optimizer.param_groups)#[0]['lr'])\n",
    "\n",
    "\n",
    "            if ((i + 1) % self.select_every == 0):\n",
    "\n",
    "                cached_state_dict = copy.deepcopy(main_model.state_dict())\n",
    "                clone_dict = copy.deepcopy(cached_state_dict)\n",
    "\n",
    "                alpha_orig = alphas.detach().clone()#copy.deepcopy(alphas)\n",
    "\n",
    "                '''alpha_orig.requires_grad = False\n",
    "                alpha_orig = alpha_orig*((constraint >0).float())\n",
    "                alpha_orig.requires_grad = True'''\n",
    "\n",
    "\n",
    "                fsubset_d.lr = main_optimizer.param_groups[0]['lr']*mul#,1e-4)\n",
    "\n",
    "                state_values = list(main_optimizer.state.values())\n",
    "                step = 0#state_values[0]['step']\n",
    "\n",
    "                w_exp_avg = torch.zeros(x_trn.shape[1]+1,device=self.device)\n",
    "                #torch.cat((state_values[0]['exp_avg'].view(-1),state_values[1]['exp_avg']))\n",
    "                w_exp_avg_sq = torch.zeros(x_trn.shape[1]+1,device=self.device)\n",
    "                #torch.cat((state_values[0]['exp_avg_sq'].view(-1),state_values[1]['exp_avg_sq']))\n",
    "\n",
    "                state_values = list(dual_optimizer.state.values())\n",
    "\n",
    "                a_exp_avg = torch.zeros(1,device=self.device)\n",
    "                #state_values[0]['exp_avg']\n",
    "                a_exp_avg_sq = torch.zeros(1,device=self.device)\n",
    "                #state_values[0]['exp_avg_sq']\n",
    "\n",
    "                #print(exp_avg,exp_avg_sq)\n",
    "\n",
    "                d_sub_idxs = fsubset_d.return_subset(clone_dict,sub_epoch,current_idxs,alpha_orig,bud,\\\n",
    "                    train_batch_size,step,w_exp_avg,w_exp_avg_sq,a_exp_avg,a_exp_avg_sq)#,main_optimizer,dual_optimizer)\n",
    "\n",
    "                '''clone_dict = copy.deepcopy(cached_state_dict)\n",
    "                alpha_orig = copy.deepcopy(alphas)\n",
    "\n",
    "                sub_idxs = fsubset.return_subset(clone_dict,sub_epoch,sub_idxs,alpha_orig,bud,\\\n",
    "                    train_batch_size)\n",
    "                print(sub_idxs[:10])'''\n",
    "\n",
    "                current_idxs = d_sub_idxs\n",
    "\n",
    "                d_sub_idxs = list(np.array(sub_rand_idxs)[d_sub_idxs])\n",
    "\n",
    "                new_ele = set(d_sub_idxs).difference(set(sub_idxs))\n",
    "                #print(len(new_ele),0.1*bud)\n",
    "\n",
    "                if len(new_ele) > 0.1*bud:\n",
    "                    main_optimizer = torch.optim.Adam([\n",
    "                    {'params': main_model.parameters()}], lr=main_optimizer.param_groups[0]['lr'])\n",
    "                    #max(main_optimizer.param_groups[0]['lr'],0.001))\n",
    "\n",
    "                    dual_optimizer = torch.optim.Adam([{'params': alphas}], lr=self.learning_rate)\n",
    "\n",
    "                    #mul=1\n",
    "                    stop_count = 0\n",
    "                    lr_count = 0\n",
    "\n",
    "                sub_idxs = d_sub_idxs\n",
    "\n",
    "                sub_idxs.sort()\n",
    "\n",
    "                print(\"First 10 subset indices: \", sub_idxs[:10])\n",
    "                np.random.seed(42)\n",
    "                np_sub_idxs = np.array(sub_idxs)\n",
    "                np.random.shuffle(np_sub_idxs)\n",
    "                loader_tr = DataLoader(CustomDataset(x_trn[np_sub_idxs], y_trn[np_sub_idxs],\\\n",
    "                        transform=None),shuffle=False,batch_size=train_batch_size)\n",
    "\n",
    "                main_model.load_state_dict(cached_state_dict)\n",
    "\n",
    "            if abs(prev_loss - temp_loss) <= 1e-1*mul or abs(temp_loss - prev_loss2) <= 1e-1*mul:\n",
    "                #print(main_optimizer.param_groups[0]['lr'])\n",
    "                #print('lr',i)\n",
    "                lr_count += 1\n",
    "                if lr_count == 10:\n",
    "                    # print(i,\"Reduced\",mul)\n",
    "                    # print(prev_loss,temp_loss,alphas)\n",
    "                    scheduler.step()\n",
    "                    mul/=10\n",
    "                    lr_count = 0\n",
    "            else:\n",
    "                lr_count = 0\n",
    "\n",
    "            '''if (abs(prev_loss - temp_loss) <= 1e-3 or abs(temp_loss - prev_loss2) <= 1e-3) and\\\n",
    "                 stop_count >= 5:\n",
    "                print(i,prev_loss,temp_loss,constraint)\n",
    "                break \n",
    "            elif abs(prev_loss - temp_loss) <= 1e-3 or abs(temp_loss - prev_loss2) <= 1e-3:\n",
    "                #print(prev_loss,temp_loss)\n",
    "                stop_count += 1\n",
    "            else:\n",
    "                stop_count = 0'''\n",
    "\n",
    "            '''if constraint <= 0 and (stop_count >= 2 or (i + 1) % select_every == 0): #10:\n",
    "                print(i,constraint)\n",
    "                break\n",
    "            elif constraint <= 0:\n",
    "                #print(alphas,constraint,stop_count)\n",
    "                stop_count += 1\n",
    "            else:\n",
    "                stop_count = 0'''\n",
    "\n",
    "\n",
    "            '''if i>=2000:\n",
    "                break'''\n",
    "\n",
    "            prev_loss2 = prev_loss\n",
    "            prev_loss = temp_loss\n",
    "            #i +=1\n",
    "\n",
    "        self.subset_idx = sub_idxs\n",
    "        #print(constraint)\n",
    "        #print(alphas)\n",
    "        no_red_error = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "        #loader_tst = DataLoader(CustomDataset(x_tst, y_tst,transform=None),shuffle=False,\\\n",
    "        #    batch_size=batch_size)\n",
    "\n",
    "        main_model.eval()\n",
    "\n",
    "        l = [torch.flatten(p) for p in main_model.parameters()]\n",
    "        flat = torch.cat(l)\n",
    "\n",
    "        # print(func_name,len(sub_idxs),file=modelfile)\n",
    "        # print(flat,file=modelfile)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            '''full_trn_out = main_model(x_trn)\n",
    "            full_trn_loss = criterion(full_trn_out, y_trn)\n",
    "            sub_trn_out = main_model(x_trn[idxs])\n",
    "            sub_trn_loss = criterion(sub_trn_out, y_trn[idxs])\n",
    "            print(\"\\nFinal SubsetTrn and FullTrn Loss:\", sub_trn_loss.item(),full_trn_loss.item(),file=logfile)'''\n",
    "\n",
    "            #val_loss = 0.\n",
    "            for batch_idx in list(loader_val.batch_sampler):\n",
    "\n",
    "\n",
    "                inputs, targets = loader_val.dataset[batch_idx]\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                val_out = main_model(inputs)\n",
    "                '''if is_time:\n",
    "                    val_out = sc_trans.inverse_transform(val_out.cpu().numpy())\n",
    "                    val_out = torch.from_numpy(val_out).float()'''\n",
    "\n",
    "                if batch_idx[0] == 0:\n",
    "                    e_val_loss = no_red_error(val_out, targets)\n",
    "\n",
    "                else:\n",
    "                    batch_val_loss = no_red_error(val_out, targets)\n",
    "                    e_val_loss = torch.cat((e_val_loss, batch_val_loss),dim= 0)\n",
    "\n",
    "            #val_loss /= len(loader_val.batch_sampler)\n",
    "            self.val_loss = torch.mean(e_val_loss)\n",
    "            # print(list(e_val_loss.cpu().numpy()),file=modelfile)\n",
    "\n",
    "            if(default == True):\n",
    "                for batch_idx in list(loader_tst.batch_sampler):\n",
    "\n",
    "                    inputs, targets = loader_tst.dataset[batch_idx]\n",
    "                    inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                    outputs = main_model(inputs)\n",
    "                    '''if is_time:\n",
    "                        outputs = sc_trans.inverse_transform(outputs.cpu().numpy())\n",
    "                        outputs = torch.from_numpy(outputs).float()'''\n",
    "                    #test_loss += criterion(outputs, targets)\n",
    "\n",
    "                    if batch_idx[0] == 0:\n",
    "                        e_tst_loss = no_red_error(outputs, targets)\n",
    "\n",
    "                    else:\n",
    "                        batch_tst_loss = no_red_error(outputs, targets)\n",
    "                        e_tst_loss = torch.cat((e_tst_loss, batch_tst_loss),dim= 0)\n",
    "\n",
    "                #test_loss /= len(loader_tst.batch_sampler)    \n",
    "                self.test_loss = torch.mean(e_tst_loss)\n",
    "                self.test_loss_std = torch.std(e_tst_loss)\n",
    "                # print(list(e_tst_loss.cpu().numpy()),file=modelfile)    \n",
    "       \n",
    "    def val_loss(self):\n",
    "        return self.val_loss.cpu().numpy()\n",
    "    \n",
    "    def test_loss(self):\n",
    "        return (self.test_loss.cpu().numpy(), self.test_loss_std.cpu().numpy())\n",
    "\n",
    "    def return_subset(self):\n",
    "        return np.array(self.subset_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOJ8w1GSyD8O7ZDRiosrRQA",
   "collapsed_sections": [],
   "name": "CS769 Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
